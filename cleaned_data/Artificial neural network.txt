neural network redirects uses neural network disambiguation


machine learning
data mining







problems



classification
clustering
regression
anomaly detection
association rules
reinforcement learning
structured prediction
feature engineering
feature learning
online learning
semisupervised learning
unsupervised learning
learning rank
grammar induction










supervised learning
classification regression




decision trees
ensembles bagging boosting random forest

linear regression
naive bayes
neural networks
logistic regression
perceptron
relevance vector machine
support vector machine









clustering



birch
cure
hierarchical
kmeans
expectation–maximization

dbscan
optics
meanshift









dimensionality reduction



factor analysis





tsne









structured prediction



graphical models bayes









anomaly detection




local outlier factor









neural nets



autoencoder
deep learning
multilayer perceptron

restricted boltzmann machine

convolutional neural network









reinforcement learning



qlearning
sarsa
temporal difference









theory



biasvariance dilemma
computational learning theory
empirical risk minimization
occam learning
learning
statistical learning
theory









machinelearning venues



nips
icml

jmlr
arxivcslg









related articles



list datasets machinelearning research
outline machine learning









machine learning portal



















artificial neural network interconnected group nodes akin vast network neurons brain circular node represents artificial neuron arrow represents connection output artificial neuron input another


artificial neural networks anns connectionist systems computing systems inspired biological neural networks constitute animal brains systems learn progressively improve performance tasks considering examples generally without taskspecific programming example image recognition might learn identify images contain cats analyzing example images manually labeled using results identify cats images without priori knowledge cats tails whiskers catlike faces instead evolve relevant characteristics learning material process
based collection connected units nodes called artificial neurons analogous biological neurons animal brain connection analogous synapse artificial neurons transmit signal another artificial neuron receives signal process signal artificial neurons connected
common implementations signal connection artificial neurons real number output artificial neuron calculated nonlinear function inputs artificial neurons connections typically weight adjusts learning proceeds weight increases decreases strength signal connection artificial neurons threshold aggregate signal crosses threshold signal sent typically artificial neurons organized layers different layers perform different kinds transformations inputs signals travel first input last output layer possibly traversing layers multiple times
original goal approach solve problems human brain would time attention focused matching specific mental abilities leading deviations biology anns used variety tasks including computer vision speech recognition machine translation social network filtering playing board video games medical diagnosis



contents


history

hebbian learning
backpropagation
hardwarebased designs
contests
convolutional networks


models

components artificial neural network

neurons
connections weights
propagation function
learning rule


neural networks functions
learning

choosing cost function
backpropagation


learning paradigms

supervised learning
unsupervised learning
reinforcement learning
convergent recursive learning algorithm


learning algorithms


variants

group method data handling
convolutional neural networks
long shortterm memory
deep reservoir computing
deep belief networks
large memory storage retrieval neural networks
stacked denoising autoencoders
deep stacking networks
tensor deep stacking networks
spikeandslab rbms
compound hierarchicaldeep models
deep predictive coding networks
networks separate memory structures

3131 lstmrelated differentiable memory structures

31311 neural turing machines


3132 semantic hashing
3133 memory networks
3134 pointer networks
3135 encoder–decoder networks




multilayer kernel machine

applications

neuroscience

types models
networks memory




theoretical properties

computational power
capacity
convergence
generalization statistics


criticism

training issues
theoretical issues
hardware issues
practical counterexamples criticisms
hybrid approaches


types
gallery
also
references
bibliography
external links



historyedit
warren mcculloch walter pitts1 1943 created computational model neural networks based mathematics algorithms called threshold logic model paved neural network research split approaches approach focused biological processes brain focused application neural networks artificial intelligence work work nerve networks link finite automata2
hebbian learningedit
late 1940s hebb3 created learning hypothesis based mechanism neural plasticity became known hebbian learning hebbian learning unsupervised learning evolved models long term potentiation researchers started applying ideas computational models 1948 turings btype machines
farley clark4 1954 first used computational machines called calculators simulate hebbian network neural network computational machines created rochester holland habit duda 19565
rosenblatt6 1958 created perceptron algorithm pattern recognition mathematical notation rosenblatt described circuitry basic perceptron exclusiveor circuit could processed neural networks time7
1959 biological model proposed nobel laureates hubel wiesel based discovery types cells primary visual cortex simple cells complex cells8
first functional networks many layers published ivakhnenko lapa 1965 becoming group method data handling91011
neural network research stagnated machine learning research minsky papert 196912 discovered issues computational machines processed neural networks first basic perceptrons incapable processing exclusiveor circuit second computers didnt enough processing power effectively handle work required large neural networks neural network research slowed computers achieved greater processing power
much artificial intelligence focused highlevel symbolic models processed using algorithms characterized example expert systems knowledge embodied ifthen rules late 1980s research expanded lowlevel subsymbolic machine learning characterized knowledge embodied parameters cognitive modelcitation needed
backpropagationedit
trigger renewed interest neural networks learning werboss 1975 backpropagation algorithm effectively solved exclusiveor problem generally accelerated training multilayer networks backpropagation distributed error term back layers modifying weights node7
mid1980s parallel distributed processing became popular name connectionism rumelhart mcclelland 1986 described connectionism simulate neural processes13
support vector machines much simpler methods linear classifiers gradually overtook neural networks machine learning popularity
earlier challenges training deep neural networks successfully addressed methods unsupervised pretraining available computing power increased gpus distributed computing neural networks deployed large scale particularly image visual recognition problems became known deep learning although deep learning strictly synonymous deep neural networks
1992 maxpooling introduced help least shift invariance tolerance deformation object recognition141516
vanishing gradient problem affects manylayered feedforward networks used backpropagation also recurrent neural networks rnns1718 errors propagate layer layer shrink exponentially number layers impeding tuning neuron weights based errors particularly affecting deep networks
overcome problem schmidhuber adopted multilevel hierarchy networks 1992 pretrained level time unsupervised learning finetuned backpropagation19 behnke 2003 relied sign gradient rprop20 problems image reconstruction face localization
hinton 2006 proposed learning highlevel representation using successive layers binary realvalued latent variables restricted boltzmann machine21 model layer sufficiently many layers learned deep architecture used generative model reproducing data sampling model ancestral pass level feature activations2223 2012 dean created network learned recognize higherlevel concepts cats watching unlabeled images taken youtube videos24
hardwarebased designsedit
computational devices created cmos biophysical simulation neuromorphic computing nanodevices25 large scale principal components analyses convolution create class neural computing fundamentally analog rather digital even though first implementations digital devices26 ciresan colleagues 201027 schmidhubers group showed despite vanishing gradient problem gpus makes backpropagation feasible manylayered feedforward neural networks
contestsedit
2009 2012 recurrent neural networks deep feedforward neural networks developed schmidhubers research group eight international competitions pattern recognition machine learning2829 example bidirectional multidimensional long shortterm memory lstm30313233 graves three competitions connected handwriting recognition 2009 international conference document analysis recognition icdar without prior knowledge three languages learned3231
ciresan colleagues pattern recognition contests including ijcnn 2011 traffic sign recognition competition34 isbi 2012 segmentation neuronal structures electron microscopy stacks challenge35 others neural networks first pattern recognizers achieve humancompetitive even superhuman performance36 benchmarks traffic sign recognition ijcnn 2012 mnist handwritten digits problem
researchers demonstrated 2010 deep neural networks interfaced hidden markov model contextdependent states define neural network output layer drastically reduce errors largevocabulary speech recognition tasks voice search
gpubased implementations37 approach many pattern recognition contests including ijcnn 2011 traffic sign recognition competition34 isbi 2012 segmentation neuronal structures stacks challenge38 imagenet competition39 others
deep highly nonlinear neural architectures similar neocognitron40 standard architecture vision41 inspired simple complex cells pretrained unsupervised methods hinton4222 team 2012 contest sponsored merck design software help find molecules might identify drugs43
convolutional networksedit
2011 state deep learning feedforward networks alternated convolutional layers maxpooling layers3744 topped several fully sparsely connected layers followed final classification layer learning usually done without unsupervised pretraining
supervised deep learning methods first achieve humancompetitive performance certain tasks36
anns able guarantee shift invariance deal small large natural objects large cluttered scenes invariance extended beyond shift annlearned concepts location type object class label scale lighting others realized developmental networks dns45 whose embodiments wherewhat networks wwn1 200846 wwn7 201347
modelsedit






section confusing unclear readers please help clarify section might discussion talk page april 2017 learn remove template message



artificial neural network network simple elements called neurons receive input change internal state activation according input produce output depending input activation network forms connecting output certain neurons input neurons forming directed weighted graph weights well functions compute activation modified process called learning governed learning rule48
components artificial neural networkedit
neuronsedit
neuron label






displaystyle

receiving input














displaystyle

predecessor neurons consists following components48

activation














displaystyle

depending discrete time parameter
possibly threshold











displaystyle theta

stays fixed unless changed learning function
activation function






displaystyle

computes activation given time








displaystyle
















displaystyle













displaystyle theta

input














displaystyle

giving rise relation

















































displaystyle ajt1fajtpjttheta




output function













displaystyle fout

computing output activation





































displaystyle ojtfoutajt



often output function simply identity function
input neuron predecessor serves input interface whole network similarly output neuron successor thus serves output interface whole network
connections weightsedit
network consists connections connection transferring output neuron






displaystyle

input neuron






displaystyle

sense






displaystyle

predecessor






displaystyle








displaystyle

successor






displaystyle

connection assigned weight












displaystyle


propagation functionedit
propagation function computes input














displaystyle

neuron






displaystyle

outputs














displaystyle

predecessor neurons typically form48







































displaystyle pjtsum ioitwij



learning ruleedit
learning rule rule algorithm modifies parameters neural network order given input network produce favored output learning process typically amounts modifying weights thresholds variables within network48
neural networks functionsedit
also graphical models
neural network models viewed simple mathematical models defining function












displaystyle textstyle fxrightarrow

distribution








displaystyle textstyle










displaystyle textstyle










displaystyle textstyle

sometimes models intimately associated particular learning rule common phrase model really definition class functions members class obtained varying parameters connection weights specifics architecture number neurons connectivity
mathematically neurons network function











displaystyle textstyle

defined composition functions
















displaystyle textstyle

decomposed functions conveniently represented network structure arrows depicting dependencies functions widely used type composition nonlinear weighted






































displaystyle textstyle fxkleftsum iwigixright










displaystyle textstyle

commonly referred activation function49 predefined function hyperbolic tangent sigmoid function softmax function rectifier function important characteristic activation function provides smooth transition input values change small change input produces small change output following refers collection functions













displaystyle textstyle

vector

































displaystyle textstyle gg1g2ldots






dependency graph


figure depicts decomposition








displaystyle textstyle

dependencies variables indicated arrows interpreted ways
first view functional view input








displaystyle textstyle

transformed 3dimensional vector








displaystyle textstyle

transformed 2dimensional vector








displaystyle textstyle

finally transformed








displaystyle textstyle

view commonly encountered context optimization
second view probabilistic view random variable













displaystyle textstyle

depends upon random variable













displaystyle textstyle

depends upon













displaystyle textstyle

depends upon random variable








displaystyle textstyle

view commonly encountered context graphical models
views largely equivalent either case particular architecture components individual layers independent components








displaystyle textstyle

independent given input








displaystyle textstyle

naturally enables degree parallelism implementation




separate depictions recurrent dependency graph


networks previous commonly called feedforward graph directed acyclic graph networks cycles commonly called recurrent networks commonly depicted manner shown figure








displaystyle textstyle

shown dependent upon however implied temporal dependence shown
learningedit
also mathematical optimization estimation theory machine learning
possibility learning attracted interest neural networks given specific task solve class functions








displaystyle textstyle

learning means using observations find















displaystyle textstyle

solves task optimal sense
entails defining cost function














displaystyle textstyle cfrightarrow mathbb

optimal solution













displaystyle textstyle























displaystyle textstyle cfleq













displaystyle textstyle forall

solution cost less cost optimal solution mathematical optimization
cost function








displaystyle textstyle

important concept learning measure away particular solution optimal solution problem solved learning algorithms search solution space find function smallest possible cost
applications solution data dependent cost must necessarily function observations otherwise model would relate data frequently defined statistic approximations made simple example consider problem finding model








displaystyle textstyle

minimizes



























displaystyle textstyle celeftfxy2right

data pairs












displaystyle textstyle

drawn distribution












displaystyle textstyle mathcal

practical situations would








displaystyle textstyle

samples












displaystyle textstyle mathcal

thus example would minimize
























































displaystyle textstyle cfrac 1nsum i1nfxiyi2

thus cost minimized sample data rather entire distribution











displaystyle textstyle nrightarrow infty

form online machine learning must used cost reduced example seen online machine learning often used












displaystyle textstyle mathcal

fixed useful case distribution changes slowly time neural network methods form online machine learning frequently used finite datasets
choosing cost functionedit
possible define cost function frequently particular cost function used either desirable properties convexity arises naturally particular formulation problem probabilistic formulation posterior probability model used inverse cost ultimately cost function depends task
backpropagationedit
main article backpropagation
discriminatively trained standard backpropagation algorithm backpropagation method calculate gradient loss function produces cost associated given state respect weights
basics continuous backpropagation9505152 derived context control theory kelley53 1960 bryson 196154 using principles dynamic programming 1962 dreyfus published simpler derivation based chain rule55 bryson described multistage dynamic system optimization method 19695657 1970 linnainmaa finally published general method automatic differentiation discrete connected networks nested differentiable functions5859 corresponds modern version backpropagation efficient even networks sparse9506061 1973 dreyfus used backpropagation adapt parameters controllers proportion error gradients62 1974 werbos mentioned possibility applying principle anns63 1982 applied linnainmaas method neural networks widely used today5064 1986 rumelhart hinton williams noted method generate useful internal representations incoming data hidden layers neural networks65 1993 first9 international pattern recognition contest backpropagation66
weight updates backpropagation done stochastic gradient descent using following equation























































displaystyle wijt1wijteta frac partial cpartial wijxi










displaystyle

learning rate






displaystyle

cost loss function









displaystyle

stochastic term choice cost function depends factors learning type supervised unsupervised reinforcement activation function example performing supervised learning multiclass classification problem common choices activation function cost function softmax function cross entropy function respectively softmax function defined














































displaystyle pjfrac expxjsum kexpxk













displaystyle

represents class probability output unit






displaystyle













displaystyle













displaystyle

represent total input units






displaystyle








displaystyle

level respectively cross entropy defined






























displaystyle csum jdjlogpj













displaystyle

represents target probability output unit






displaystyle













displaystyle

probability output






displaystyle

applying activation function67
used output object bounding boxes form binary mask also used multiscale regression increase localization precision dnnbased regression learn features capture geometric information addition serving good classifier remove requirement explicitly model parts relations helps broaden variety objects learned model consists multiple layers rectified linear unit activation function nonlinear transformation layers convolutional others fully connected every convolutional layer additional pooling network trained minimize error predicting mask ranging entire training containing bounding boxes represented masks
alternatives backpropagation include extreme learning machines68 noprop networks69 training without backtracking70 weightless networks7172 nonconnectionist neural networks
learning paradigmsedit
three major learning paradigms correspond particular learning task supervised learning unsupervised learning reinforcement learning
supervised learningedit
supervised learning uses example pairs


















displaystyle xyxin xyin

find function










displaystyle fxrightarrow

allowed class functions matches examples words wish infer mapping implied data cost function related mismatch mapping data implicitly contains prior knowledge problem domain73
commonly used cost meansquared error tries minimize average squared error networks output









displaystyle

target value






displaystyle

example pairs minimizing cost using gradient descent class neural networks called multilayer perceptrons produces backpropagation algorithm training neural networks
tasks fall within paradigm supervised learning pattern recognition also known classification regression also known function approximation supervised learning paradigm also applicable sequential data hand writing speech gesture recognition thought learning teacher form function provides continuous feedback quality solutions obtained thus
unsupervised learningedit
unsupervised learning data








displaystyle textstyle

given cost function minimized function data








displaystyle textstyle

networks output








displaystyle textstyle


cost function dependent task model domain priori assumptions implicit properties model parameters observed variables
trivial example consider model













displaystyle textstyle










displaystyle textstyle

constant cost

























displaystyle textstyle cexfx2

minimizing cost produces value








displaystyle textstyle

equal mean data cost function much complicated form depends application example compression could related mutual information








displaystyle textstyle













displaystyle textstyle

whereas statistical modeling could related posterior probability model given data note examples quantities would maximized rather minimized
tasks fall within paradigm unsupervised learning general estimation problems applications include clustering estimation statistical distributions compression filtering
reinforcement learningedit
also stochastic control
reinforcement learning data








displaystyle textstyle

usually given generated agents interactions environment point time








displaystyle textstyle

agent performs action













displaystyle textstyle

environment generates observation













displaystyle textstyle

instantaneous cost













displaystyle textstyle

according usually unknown dynamics discover policy selecting actions minimizes measure longterm cost expected cumulative cost environments dynamics longterm cost policy usually unknown estimated
formally environment modeled markov decision process states




























displaystyle textstyle s1snin

actions




























displaystyle textstyle a1amin

following probability distributions instantaneous cost distribution

























displaystyle textstyle pctst

observation distribution

























displaystyle textstyle pxtst

transition


































displaystyle textstyle pst1stat

policy defined conditional distribution actions given observations taken together define markov chain discover policy minimizes cost
anns frequently used reinforcement learning part overall algorithm7475 dynamic programming coupled anns giving neurodynamic programming bertsekas tsitsiklis76 applied multidimensional nonlinear problems involved vehicle routing77 natural resources management7879 medicine80 ability anns mitigate losses accuracy even reducing discretization grid density numerically approximating solution original control problems
tasks fall within paradigm reinforcement learning control problems games sequential decision making tasks
convergent recursive learning algorithmedit
learning method specially designed cerebellar model articulation controller cmac neural networks 2004 recursive least squares algorithm introduced train cmac neural network online81 algorithm converge step update weights step input data initially algorithm computational complexity based decomposition recursive learning algorithm simplified on82
learning algorithmsedit
also machine learning
training neural network model essentially means selecting model allowed models bayesian framework determining distribution allowed models minimizes cost numerous algorithms available training neural network models viewed straightforward application optimization theory statistical estimation
employ form gradient descent using backpropagation compute actual gradients done simply taking derivative cost function respect network parameters changing parameters gradientrelated direction backpropagation training algorithms fall three categories

steepest descent variable learning rate momentum resilient backpropagation
quasinewton broydenfletchergoldfarbshanno step secant
levenbergmarquardt conjugate gradient fletcherreeves update polakribiére update powellbeale restart scaled conjugate gradient83

evolutionary methods84 gene expression programming85 simulated annealing86 expectationmaximization nonparametric methods particle swarm optimization87 methods training neural networks
variantsedit
group method data handlingedit
main article group method data handling
group method data handling gmdh88 features fully automatic structural parametric model optimization node activation functions kolmogorovgabor polynomials permit additions multiplications used deep feedforward multilayer perceptron eight layers89 supervised learning network grows layer layer layer trained regression analysis useless items detected using validation pruned regularization size depth resulting network depends task90
convolutional neural networksedit
main article convolutional neural network
convolutional neural network class deep feedforward networks composed convolutional layers fully connected layers matching typical anns uses tied weights pooling layers particular maxpooling15 often structured fukushimas convolutional architecture91 architecture allows cnns take advantage structure input data
cnns suitable processing visual twodimensional data9293 shown superior results image speech applications trained standard backpropagation cnns easier train regular deep feedforward neural networks many fewer parameters estimate94 examples applications computer vision include deepdream95
long shortterm memoryedit
main article long shortterm memory
long shortterm memory lstm networks rnns avoid vanishing gradient problem96 lstm normally augmented recurrent gates called forget gates97 lstm networks prevent backpropagated errors vanishing exploding17 instead errors flow backwards unlimited numbers virtual layers spaceunfolded lstm lstm learn deep learning tasks9 require memories events happened thousands even millions discrete time steps problemspecific lstmlike topologies evolved98 lstm handle long delays signals high frequency components
stacks lstm rnns99 trained connectionist temporal classification ctc100 find weight matrix maximizes probability label sequences training given corresponding input sequences achieves alignment recognition
2003 lstm started become competitive traditional speech recognizers101 2007 combination achieved first good results speech data102 2009 ctctrained lstm first pattern recognition contests several competitions connected handwriting recognition932 2014 baidu used ctctrained rnns break switchboard hub500 speech recognition benchmark without traditional speech processing methods103 lstm also improved largevocabulary speech recognition104105 texttospeech synthesis106 google android50107 photoreal talking heads108 2015 googles speech recognition experienced improvement ctctrained lstm109
lstm became popular natural language processing unlike previous models based hmms similar concepts lstm learn recognise contextsensitive languages110 lstm improved machine translation111 language modeling112 multilingual language processing113 lstm combined cnns improved automatic image captioning114
deep reservoir computingedit
main article reservoir computing
deep reservoir computing deep echo state networks deepesns115116 provide framework efficiently trained models hierarchical processing temporal data enabling investigation inherent role layered compositionclarification needed
deep belief networksedit
main article deep belief network




restricted boltzmann machine fully connected visible hidden units note hiddenhidden visiblevisible connections


deep belief network probabilistic generative model made multiple layers hidden units considered composition simple learning modules make layer117
used generatively pretrain using learned weights initial weights backpropagation discriminative algorithms tune weights particularly helpful training data limited poorly initialized weights significantly hinder model performance pretrained weights region weight space closer optimal weights randomly chosen allows improved modeling faster convergence finetuning phase118
large memory storage retrieval neural networksedit
large memory storage retrieval neural networks lamstar119120 fast deep learning neural networks many layers many filters simultaneously filters nonlinear stochastic logic nonstationary even nonanalytical biologically motivated learn continuously
lamstar neural network serve dynamic neural network spatial time domains speed provided hebbian linkweights121 integrate various usually different filters preprocessing functions many layers dynamically rank significance various layers functions relative given learning task grossly imitates biological learning integrates various preprocessors cochlea retina cortexes auditory visual various regions deep learning capability enhanced using inhibition correlation ability cope incomplete data lost neurons layers even amidst task fully transparent link weights linkweights allow dynamic determination innovation redundancy facilitate ranking layers filters individual neurons relative task
lamstar applied many domains including medical122123124 financial predictions125 adaptive filtering noisy speech unknown noise126 stillimage recognition127 video image recognition128 software security129 adaptive control nonlinear systems130 lamstar much faster learning speed somewhat lower error rate based relufunction filters pooling comparative studies131
applications demonstrate delving aspects data hidden shallow learning networks human senses cases predicting onset sleep apnea events123 electrocardiogram fetus recorded skinsurface electrodes placed mothers abdomen early pregnancy124 financial prediction119 blind filtering noisy speech126
lamstar proposed 1996 patent 5920852 developed graupe kordylewski 19972002132133134 modified version known lamstar developed schneider graupe 2008135136
stacked denoising autoencodersedit
auto encoder idea motivated concept good representation example classifier good representation defined yields betterperforming classifier
encoder deterministic mapping











displaystyle ftheta

transforms input vector hidden representation














displaystyle theta boldsymbol










displaystyle boldsymbol

weight matrix offset vector bias decoder maps back hidden representation reconstructed input











displaystyle gtheta

whole process auto encoding compare reconstructed input original minimize error make reconstructed value close possible original
stacked denoising auto encoders partially corrupted output cleaned denoised idea introduced 2010 vincent al137 specific approach good representation good representation obtained robustly corrupted input useful recovering corresponding clean input implicit definition following ideas

higher level representations relatively stable robust input corruption
necessary extract features useful representation input distribution

algorithm starts stochastic mapping








displaystyle boldsymbol















displaystyle tilde boldsymbol





























displaystyle qdtilde boldsymbol xboldsymbol

corrupting step corrupted input













displaystyle tilde boldsymbol

passes basic autoencoder process mapped hidden representation










































displaystyle boldsymbol yftheta tilde boldsymbol xsboldsymbol wtilde boldsymbol

hidden representation reconstruct




















displaystyle boldsymbol zgtheta boldsymbol

last stage minimization algorithm runs order close possible uncorrupted input








displaystyle boldsymbol

reconstruction error




















displaystyle lhboldsymbol xboldsymbol

might either crossentropy loss affinesigmoid decoder squared error loss affine decoder137
order make deep architecture auto encoders stack138 encoding function











displaystyle ftheta

first denoising auto encoder learned used uncorrupt input corrupted input second level trained137
stacked auto encoder trained output used input supervised learning algorithm support vector machine classifier multiclass logistic regression137
deep stacking networksedit
deep stacking network dsn139 deep convex network based hierarchy blocks simplified neural network modules introduced 2011 deng dong140 formulates learning convex optimization problem closedform solution emphasizing mechanisms similarity stacked generalization141 block simple module easy train supervised fashion without backpropagation entire blocks142
block consists simplified multilayer perceptron single hidden layer hidden layer logistic sigmoidal units output layer linear units connections layers represented weight matrix inputtohiddenlayer connections weight matrix target vectors form columns matrix input data vectors form columns matrix matrix hidden units























displaystyle boldsymbol hsigma boldsymbol wtboldsymbol

modules trained order lowerlayer weights known stage function performs elementwise logistic sigmoid operation block estimates final label class estimate concatenated original input form expanded input next block thus input first block contains original data downstream blocks input adds output preceding blocks learning upperlayer weight matrix given weights network formulated convex optimization problem
























































displaystyle utfboldsymbol utboldsymbol hboldsymbol



closedform solution
unlike deep architectures dbns goal discover transformed feature representation structure hierarchy kind architecture makes parallel learning straightforward batchmode optimization problem purely discriminative tasks dsns perform better conventional dbns139
tensor deep stacking networksedit
architecture extension offers important improvements uses higherorder information covariance statistics transforms nonconvex problem lowerlayer convex subproblem upperlayer143 tdsns covariance statistics bilinear mapping distinct sets hidden units layer predictions thirdorder tensor
parallelization scalability considered seriously conventional dnns144145146 learning dsns tdsns done batch mode allow parallelization140139 parallelization allows scaling design larger deeper architectures data sets
basic architecture suitable diverse tasks classification regression
spikeandslab rbmsedit
need deep learning realvalued inputs gaussian restricted boltzmann machines spikeandslab ssrbm models continuousvalued inputs strictly binary latent variables147 similar basic rbms variants spikeandslab bipartite graph like grbms visible units input realvalued difference hidden layer hidden unit binary spike variable realvalued slab variable spike discrete probability mass zero slab density continuous domain148 mixture forms prior149
extension ssrbm called µssrbm provides extra modeling capacity using additional terms energy function terms enables model form conditional distribution spike variables marginalizing slab variables given observation
compound hierarchicaldeep modelsedit
compound hierarchicaldeep models compose deep networks nonparametric bayesian models features learned using deep architectures dbns150 dbms151 deep auto encoders152 convolutional variants153154 ssrbms148 deep coding networks155 dbns sparse feature learning156 rnns157 conditional dbns158 denoising auto encoders159 provides better representation allowing faster learning accurate classification highdimensional data however architectures poor learning novel classes examples network units involved representing input distributed representation must adjusted together high degree freedom limiting degree freedom reduces number parameters learn facilitating learning classes examples hierarchical bayesian models allow learning examples example160161162163164 computer vision statistics cognitive science
compound architectures integrate characteristics deep networks compound hdpdbm architecture hierarchical dirichlet process hierarchical model incorporated architecture full generative model generalized abstract concepts flowing layers model able synthesize examples novel classes look reasonably natural levels learned jointly maximizing joint logprobability score165
three hidden layers probability visible input
















































































































































displaystyle pboldsymbol frac 1zsum hesum ijwij1nu ihj1sum jlwjl2hj1hl2sum lmwlm3hl2hm3















































displaystyle boldsymbol hboldsymbol h1boldsymbol h2boldsymbol

hidden units









































displaystyle boldsymbol w1boldsymbol w2boldsymbol

model parameters representing visiblehidden hiddenhidden symmetric interaction terms
learned model undirected model defines joint distribution






























displaystyle h1h2h3

express learned conditional model
































displaystyle h1h2h3

prior term














displaystyle



































displaystyle h1h2h3

represents conditional model viewed twolayer bias terms given states











displaystyle











































































































































































displaystyle h1h2h3frac 1zpsi h3esum ijwij1nu ihj1sum jlwjl2hj1hl2sum lmwlm3hl2hm3



deep predictive coding networksedit
deep predictive coding network dpcn predictive coding scheme uses topdown information empirically adjust priors needed bottomup inference procedure means deep locally connected generative model works extracting sparse features timevarying observations using linear dynamical model pooling strategy used learn invariant feature representations units compose form deep architecture trained greedy layerwise unsupervised learning layers constitute kind markov chain states layer depend preceding succeeding layers
dpcns predict representation layer using topdown approach using information upper layer temporal dependencies previous states166
dpcns extended form convolutional network166
networks separate memory structuresedit
integrating external memory anns dates early research distributed representations167 kohonens selforganizing maps example sparse distributed memory hierarchical temporal memory patterns encoded neural networks used addresses contentaddressable memory neurons essentially serving address encoders decoders however early controllers memories differentiable
lstmrelated differentiable memory structuresedit
apart long shortterm memory lstm approaches also added differentiable memory recurrent functions example

differentiable push actions alternative memory networks called neural stack machines168169
memory networks control networks external differentiable storage fast weights another network170
lstm forget gates171
selfreferential rnns special output units addressing rapidly manipulating rnns weights differentiable fashion internal storage172173
learning transduce unbounded memory174

neural turing machinesedit
main article neural turing machine
neural turing machines175 couple lstm networks external memory resources interact attentional processes combined system analogous turing machine differentiable endtoend allowing efficiently trained gradient descent preliminary results demonstrate neural turing machines infer simple algorithms copying sorting associative recall input output examples
semantic hashingedit
approaches represent previous experiences directly similar experience form local model often called nearest neighbourdisambiguation needed knearest neighbors methods176 deep learning useful semantic hashing177 deep graphical model wordcount vectors178 obtained large documentsclarification needed documents mapped memory addresses semantically similar documents located nearby addresses documents similar query document found accessing addresses differ bits address query document unlike sparse distributed memory operates 1000bit addresses semantic hashing works 64bit addresses found conventional computer architecture
memory networksedit
memory networks179180 another extension neural networks incorporating longterm memory longterm memory read written goal using prediction models applied context question answering longterm memory effectively acts dynamic knowledge base output textual response181
pointer networksedit
deep neural networks potentially improved deepening parameter reduction maintaining trainability training extremely deep million layers neural networks might practical cpulike architectures pointer networks182 neural randomaccess machines183 overcome limitation using external randomaccess memory components typically belong computer architecture registers pointers systems operate probability distribution vectors stored memory cells registers thus model fully differentiable trains endtoend characteristic models depth size shortterm memory number parameters altered independently unlike models like lstm whose number parameters grows quadratically memory size
encoder–decoder networksedit
encoder–decoder frameworks based neural networks highly structured input highly structured output approach arose context machine translation184185186 input output written sentences natural languages work lstm used encoder summarize source sentence summary decoded using conditional language model produce translation187 systems share building blocks gated rnns cnns trained attention mechanisms
multilayer kernel machineedit
multilayer kernel machines learning highly nonlinear functions iterative application weakly nonlinear kernels kernel principal component analysis kpca188 method unsupervised greedy layerwise pretraining step deep learning architecture189
layer








displaystyle

learns representation previous layer






displaystyle

extracting











displaystyle

principal component projection layer






displaystyle

output feature domain induced kernel sake dimensionality reduction updated representation layer supervised strategy proposed select best informative features among features extracted kpca process

rank











displaystyle

features according mutual information class labels
different values
























displaystyle mlin 1ldots

compute classification error rate knearest neighbor classifier using











displaystyle

informative features validation
value











displaystyle

classifier reached lowest error rate determines number features retain

drawbacks accompany kpca method building cells
straightforward kernel machines deep learning developed spoken language understanding190 main idea kernel machine approximate shallow neural infinite number hidden units stacking splice output kernel machine input building next higher level kernel machine number levels deep convex network hyperparameter overall system determined cross validation
useedit
using anns requires understanding characteristics

choice model depends data representation application overly complex models slow learning
learning algorithm numerous tradeoffs exist learning algorithms almost algorithm work well correct hyperparameters training particular data however selecting tuning algorithm training unseen data requires significant experimentation
robustness model cost function learning algorithm selected appropriately resulting become robust

capabilities fall within following broad categoriescitation needed

function approximation regression analysis including time series prediction fitness approximation modeling
classification including pattern sequence recognition novelty detection sequential decision making
data processing including filtering clustering blind source separation compression
robotics including directing manipulators prostheses
control including computer numerical control

applicationsedit
ability reproduce model nonlinear processes anns found many applications wide range disciplines
application areas include system identification control vehicle control trajectory prediction191 process control natural resources management quantum chemistry192 gameplaying decision making backgammon chess poker pattern recognition radar systems face identification signal classification193 object recognition sequence recognition gesture speech handwritten text recognition medical diagnosis finance automated trading systems data mining visualization machine translation social network filtering194 email spam filtering
anns used diagnose cancers including lung cancer195 prostate cancer colorectal cancer196 distinguish highly invasive cancer cell lines less invasive lines using cell shape information197198
anns used building blackbox models geoscience hydrology199200 ocean modelling coastal engineering201202 geomorphology203 examples kind
neuroscienceedit
theoretical computational neuroscience concerned theoretical analysis computational modeling biological neural systems since neural systems attempt reflect cognitive processes behavior field closely related cognitive behavioral modeling
gain understanding neuroscientists strive link observed biological processes data biologically plausible mechanisms neural processing learning biological neural network models theory statistical learning theory information theory
brain research repeatedly approaches connections connect neurons layers rather adjacent neurons layer research explored multiple signal types finer control boolean onoff variables dynamic neural networks dynamically form connections even neural units disabling others204
types modelsedit
many types models used defined different levels abstraction modeling different aspects neural systems range models shortterm behavior individual neurons205 models dynamics neural circuitry arise interactions individual neurons finally models behavior arise abstract neural modules represent complete subsystems include models longterm shortterm plasticity neural systems relations learning memory individual neuron system level
networks memoryedit
integrating external memory components artificial neural networks dates early research distributed representations167 selforganizing maps sparse distributed memory patterns encoded neural networks used memory addresses contentaddressable memory neurons essentially serving address encoders decoders
recently deep learning shown useful semantic hashing206 deep graphical model wordcount vectors178 obtained large document documents mapped memory addresses semantically similar documents located nearby addresses documents similar query document found simply accessing nearby addresses
memory networks another extension neural networks incorporating longterm memory179 longterm memory read written goal using prediction models applied context question answering longterm memory effectively acts knowledge base output textual response
neural turing machines extend capabilities deep neural networks coupling external memory resources interact attentional processes175 combined system analogous turing machine differentiable endtoend allowing efficiently trained gradient descent preliminary results demonstrate ntms infer simple algorithms copying sorting associative recall input output examples
differentiable neural computers extension outperformed neural turing machines long shortterm memory systems memory networks sequenceprocessing tasks207208209210211
theoretical propertiesedit
computational poweredit
multilayer perceptron universal function approximator proven universal approximation theorem however proof constructive regarding number neurons required network topology weights learning parameters
specific recurrent architecture rational valued weights opposed full precision real numbervalued weights full power universal turing machine212 using finite number neurons standard linear connections irrational values weights results machine superturing power213
capacityedit
models capacity property roughly corresponds ability model given function related amount information stored network notion complexitycitation needed
convergenceedit
models consistently converge single solution firstly many local minima exist depending cost function model secondly optimization method used might guarantee converge begins local minimum thirdly sufficiently large data parameters methods become impractical however cmac neural network recursive least squares algorithm introduced train algorithm guaranteed converge step81
generalization statisticsedit
applications whose goal create system generalizes well unseen examples face possibility overtraining arises convoluted overspecified systems capacity network significantly exceeds needed free parameters approaches address overtraining first crossvalidation similar techniques check presence overtraining optimally select hyperparameters minimize generalization error second form regularization concept emerges probabilistic bayesian framework regularization performed selecting larger prior probability simpler models also statistical learning theory goal minimize quantities empirical risk structural risk roughly corresponds error training predicted error unseen data overfitting




confidence analysis neural network


supervised neural networks mean squared error cost function formal statistical methods determine confidence trained model validation used estimate variance value used calculate confidence interval output network assuming normal distribution confidence analysis made statistically valid long output probability distribution stays network modified
assigning softmax activation function generalization logistic function output layer neural network softmax component componentbased neural network categorical target variables outputs interpreted posterior probabilities useful classification gives certainty measure classifications
softmax activation function





















































displaystyle yifrac exisum j1cexj




criticismedit
training issuesedit
common criticism neural networks particularly robotics require much training realworld operationcitation needed potential solutions include randomly shuffling training examples using numerical optimization algorithm take large steps changing network connections following example grouping examples socalled minibatches improving training efficiency convergence capability always ongoing research area neural network example introducing recursive least squares algorithm cmac neural network training process takes step converge81
theoretical issuesedit
neural network solved computationally difficult problems nqueens problem travelling salesman problem problem factoring large integers
fundamental objection reflect real neurons function back propagation critical part artificial neural networks although mechanism exists biological neural networks214 information coded real neurons known sensor neurons fire action potentials frequently sensor activation muscle cells pull strongly associated motor neurons receive action potentials frequently215 case relaying information sensor neuron motor neuron almost nothing principles information handled biological neural networks known
motivation behind anns necessarily strictly replicate neural function biological neural networks inspiration central claim anns therefore embodies powerful general principle processing information unfortunately general principles illdefined often claimed emergent network allows simple statistical association basic function artificial neural networks described learning recognition alexander dewdney commented result artificial neural networks somethingfornothing quality imparts peculiar aura laziness distinct lack curiosity good computing systems human hand mind intervenes solutions found magic seems learned anything216
biological brains shallow deep circuits reported brain anatomy217 displaying wide variety invariance weng218 argued brain selfwires largely according signal statistics therefore serial cascade cannot catch major statistical dependencies
hardware issuesedit
large effective neural networks require considerable computing resources219 brain hardware tailored task processing signals graph neurons simulating even simplified neuron neumann architecture compel neural network designer fill many millions database rows connections consume vast amounts memory storage furthermore designer often needs transmit signals many connections associated neurons must often matched enormous processing power time
schmidhuber notes resurgence neural networks twentyfirst century largely attributable advances hardware 1991 2015 computing power especially delivered gpgpus gpus increased around millionfold making standard backpropagation algorithm feasible training networks several layers deeper before220 parallel gpus reduce training times months days219
neuromorphic engineering addresses hardware difficulty directly constructing nonvonneumann chips directly implement neural networks circuitry another chip optimized neural network processing called tensor processing unit tpu221
practical counterexamples criticismsedit
arguments dewdneys position neural networks successfully used solve many complex diverse tasks ranging autonomously flying aircraft222 detecting credit card fraud mastering game
technology writer roger bridgman commented

neural networks instance dock hyped high heaven hasnt also could create successful without understanding worked bunch numbers captures behaviour would probability opaque unreadable tablevalueless scientific resource
spite emphatic declaration science technology dewdney seems pillory neural nets science devising trying good engineers unreadable table useful machine could read would still well worth having223

although true analyzing learned artificial neural network difficult much easier analyze learned biological neural network furthermore researchers involved exploring learning algorithms neural networks gradually uncovering general principles allow learning machine successful example local nonlocal learning shallow deep architecture224
hybrid approachesedit
advocates hybrid models combining neural networks symbolic approaches claim mixture better capture mechanisms human mind225226
typesedit
main article types artificial neural networks
artificial neural networks many variations simplest static types static components including number units number layers unit weights topology dynamic types allow change learning process latter much complicated shorten learning periods produce better results types allowrequire learning supervised operator others operate independently types operate purely hardware others purely software general purpose computers
galleryedit







singlelayer feedforward artificial neural network arrows originating













displaystyle scriptstyle

omitted clarity inputs network outputs system value output













displaystyle scriptstyle

would calculated










































displaystyle scriptstyle yqksum xiwiqbq











twolayer feedforward artificial neural network









artificial neural network









dependency graph









singlelayer feedforward artificial neural network inputs hidden outputs given position state direction outputs wheel based control values









twolayer feedforward artificial neural network inputs hidden outputs given position state direction environment values outputs thruster based control values









parallel pipeline structure cmac neural network learning algorithm converge step




alsoedit


hierarchical temporal memory

adaline
adaptive resonance theory
artificial life
associative memory
autoencoder
beam robotics
biological cybernetics
biologically inspired computing
blue brain project
catastrophic interference
cerebellar model articulation controller cmac
cognitive architecture
cognitive science
convolutional neural network
connectionist expert system
connectomics
cultured neuronal networks
deep learning
digital morphogenesis
encog
fuzzy logic
gene expression programming
genetic algorithm
genetic programming
group method data handling
habituation
situ adaptive tabulation
machine learning concepts
models neural computation
neuroevolution
neural coding
neural
neural machine translation
neural network software
neuroscience
ni1000 chip
nonlinear system identification
optical neural network
parallel constraint satisfaction processes
parallel distributed processing
radial basis function network
recurrent neural networks
selforganizing
spiking neural network
systolic array
tensor product network
time delay neural network tdnn


referencesedit


mcculloch warren walter pitts 1943 logical calculus ideas immanent nervous activity bulletin mathematical biophysics 115–133 doi101007bf02478259
kleene 1956 representation events nerve nets finite automata annals mathematics studies princeton university press 3–41 retrieved 20170617
hebb donald 1949 organization behavior york wiley isbn 9781135631901
farley clark 1954 simulation selforganizing systems digital computer transactions information theory 76–84 doi101109tit19541057468
rochester holland habit duda 1956 tests cell assembly theory action brain using large digital computer transactions information theory 80–93 doi101109tit19561056810
rosenblatt 1958 perceptron probabilistic model information storage organization brain psychological review 386–408 citeseerx 10115883775 doi101037h0042519 pmid 13602029
werbos 1975 beyond regression tools prediction analysis behavioral sciences
david hubel torsten wiesel 2005 brain visual perception story 25year collaboration oxford university press isbn 9780195176186
schmidhuber 2015 deep learning neural networks overview neural networks 85–117 arxiv14047828 doi101016jneunet201409003 pmid 25462637
ivakhnenko 1973 cybernetic predicting devices information corporation
ivakhnenko grigorʹevich lapa valentin 1967 cybernetics forecasting techniques american elsevier
minsky marvin papert seymour 1969 perceptrons introduction computational geometry press isbn 0262630222
rumelhart mcclelland james 1986 parallel distributed processing explorations microstructure cognition cambridge press isbn 9780262631105
weng ahuja huang cresceptron selforganizing neural network grows adaptively proc international joint conference neural networks baltimore maryland 576581 june 1992
weng ahuja huang learning recognition segmentation objects images proc international conf computer vision berlin germany 121128 1993
weng ahuja huang learning recognition segmentation using cresceptron international journal computer vision 105139 1997
hochreiter untersuchungen dynamischen neuronalen netzen diploma thesis institut informatik technische univ munich advisor schmidhuber 1991
hochreiter january 2001 gradient flow recurrent nets difficulty learning longterm dependencies kolen john kremer stefan field guide dynamical recurrent networks john wiley sons isbn 9780780353695 maint explicit link
schmidhuber learning complex extended sequences using principle history compression neural computation 234–242 1992
sven behnke 2003 hierarchical neural networks image interpretation lecture notes computer science 2766 springer
smolensky 1986 information processing dynamical systems foundations harmony theory rumelhart mcclelland research group parallel distributed processing explorations microstructure cognition 194–281 maint uses editors parameter link
hinton osindero 2006 fast learning algorithm deep belief nets neural computation 1527–1554 doi101162neco20061871527 pmid 16764513
hinton 2009 deep belief networks scholarpedia 5947 bibcode2009schpj45947h doi104249scholarpedia5947
andrew dean jeff 2012 building highlevel features using large scale unsupervised learning arxiv11126209 cslg
yang pickett ohlberg stewart williams 2008 memristive switching mechanism metaloxidemetal nanodevices nanotechnol 429–433 doi101038nnano2008160
strukov snider stewart williams 2008 missing memristor found nature 7191 80–83 bibcode2008natur45380s doi101038nature06932 pmid 18451858
cireşan claudiu meier ueli gambardella luca maria schmidhuber jürgen 20100921 deep simple neural nets handwritten digit recognition neural computation 3207–3220 doi101162necoa00052 issn 08997667
2012 kurzweil interview jürgen schmidhuber eight competitions deep learning team 2009–2012
bioinspired deep learning keeps winning competitions kurzweilai wwwkurzweilainet retrieved 20170616
graves alex schmidhuber jürgen offline handwriting recognition multidimensional recurrent neural networks bengio yoshua schuurmans dale lafferty john williams chris culotta aron advances neural information processing systems nips22 7–10 december 2009 vancouver neural information processing systems nips foundation 2009 545–552
graves liwicki fernandez bertolami bunke schmidhuber 2009 novel connectionist system improved unconstrained handwriting recognition ieee transactions pattern analysis machine intelligence 855–868 doi101109tpami2008137
graves alex schmidhuber jürgen 2009 bengio yoshua schuurmans dale lafferty john williams chris editork culotta aron offline handwriting recognition multidimensional recurrent neural networks neural information processing systems nips foundation 545–552
graves liwicki fernández bertolami bunke schmidhuber 2009 novel connectionist system unconstrained handwriting recognition ieee transactions pattern analysis machine intelligence 855–868 doi101109tpami2008137 issn 01628828
cireşan meier ueli masci jonathan schmidhuber jürgen august 2012 multicolumn deep neural network traffic sign classification neural networks selected papers ijcnn 2011 333–338 doi101016jneunet201202023
ciresan giusti alessandro gambardella luca schmidhuber juergen 2012 pereira burges bottou weinberger advances neural information processing systems curran associates 2843–2851
ciresan meier schmidhuber june 2012 multicolumn deep neural networks image classification 2012 ieee conference computer vision pattern recognition 3642–3649 doi101109cvpr20126248110 isbn 9781467312288
ciresan meier masci gambardella schmidhuber 2011 flexible high performance convolutional neural networks image classification international joint conference artificial intelligence doi1055919781577355168ijcai11210
ciresan giusti alessandro gambardella luca schmidhuber juergen 2012 pereira burges bottou weinberger advances neural information processing systems curran associates 2843–2851
krizhevsky alex sutskever ilya hinton geoffry 2012 imagenet classification deep convolutional neural networks nips 2012 neural information processing systems lake tahoe nevada
fukushima 1980 neocognitron selforganizing neural network model mechanism pattern recognition unaffected shift position biological cybernetics 93–202 doi101007bf00344251 pmid 7370364
riesenhuber poggio 1999 hierarchical models object recognition cortex nature neuroscience 1019–1025 doi10103814819
hinton geoffrey 20090531 deep belief networks scholarpedia 5947 bibcode2009schpj45947h doi104249scholarpedia5947 issn 19416016
markoff john november 2012 scientists promise deeplearning programs york times
martines bengio yannakakis 2013 learning deep physiological models affect ieee computational intelligence 20–33 doi101109mci20132247823
weng passed neural networks abstract well natural intelligence inns magazine 1322 2011
weng prokhorov wherewhat network assist topdown connections proc international conference development learning icdl08 monterey 2008
weng skullclosed autonomous development wwn7 dealing scales proc international conference brainmind july 27–28 east lansing michigan 2013
zell andreas 1994 chapter simulation neuronaler netze simulation neural networks german addisonwesley isbn 3893195548
machine learning dictionary
schmidhuber jürgen 2015 deep learning scholarpedia 32832 bibcode2015schpj1032832s doi104249scholarpedia32832
dreyfus stuart 19900901 artificial neural networks back propagation kelleybryson gradient procedure journal guidance control dynamics 926–928 bibcode1990jgcd13926d doi102514325422 issn 07315090
eiji mizutani stuart dreyfus kenichi nishio 2000 derivation backpropagation kelleybryson optimalcontrol gradient formula application proceedings ieee international joint conference neural networks ijcnn 2000 como italy july 2000 online
kelley henry 1960 gradient theory optimal flight paths journal 947–954 doi10251485282
arthur bryson 1961 april gradient method optimizing multistage allocation processes proceedings harvard univ symposium digital computers applications
dreyfus stuart 1962 numerical solution variational problems journal mathematical analysis applications 30–45 doi1010160022247x62900045
russell stuart norvig peter 2010 artificial intelligence modern approach prentice hall isbn 9780136042594 popular method learning multilayer networks called backpropagation
bryson arthur earl 1969 applied optimal control optimization estimation control blaisdell publishing company xerox college publishing
seppo linnainmaa 1970 representation cumulative rounding error algorithm taylor expansion local rounding errors masters thesis finnish univ helsinki
linnainmaa seppo 1976 taylor expansion accumulated rounding error numerical mathematics 146–160 doi101007bf01931367
griewank andreas 2012 invented reverse mode differentiation documenta matematica extra volume ismp 389–400
griewank andreas walther andrea 2008 evaluating derivatives principles techniques algorithmic differentiation second edition siam isbn 9780898717761
dreyfus stuart 1973 computational solution optimal control problems time ieee transactions automatic control 383–385 doi101109tac19731100330
paul werbos 1974 beyond regression tools prediction analysis behavioral sciences thesis harvard university
werbos paul 1982 applications advances nonlinear sensitivity analysis system modeling optimization springer 762–770
rumelhart david hinton geoffrey williams ronald 1986 learning representations backpropagating errors nature 6088 533–536 bibcode1986natur323533r doi101038323533a0
eric 1993 time series prediction using connectionist network internal delay lines santa institute studies sciences complexityproceedings 195195 addisonwesley publishing
hinton deng dahl mohamed jaitly senior vanhoucke nguyen november 2012 deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine 82–97 bibcode2012ispm2982h doi101109msp20122205597 issn 10535888
huang guangbin qinyu siew cheekheong 2006 extreme learning machine theory applications neurocomputing 489–501 doi101016jneucom200512126
widrow bernard 2013 noprop algorithm learning algorithm multilayer neural networks neural networks 182–188 doi101016jneunet201209020
ollivier yann charpiat guillaume 2015 training recurrent networks without backtracking arxiv150707680 csne
esann 2009
hinton 2010 practical guide training restricted boltzmann machines tech utml 2010003
ojha varun kumar abraham ajith snášel václav 20170401 metaheuristic design feedforward neural networks review decades research engineering applications artificial intelligence 97–116 doi101016jengappai201701013
dominic whitley anderson july 1991 genetic reinforcement learning neural networks ijcnn91seattle international joint conference neural networks ijcnn91seattle international joint conference neural networks seattle washington ieee doi101109ijcnn1991155315 isbn 0780301641 retrieved july 2012
hoskins himmelblau 1992 process control artificial neural networks reinforcement learning computers chemical engineering 241–251 doi101016009813549280045b
bertsekas tsitsiklis 1996 neurodynamic programming athena scientific isbn 1886529108
secomandi nicola 2000 comparing neurodynamic programming algorithms vehicle routing problem stochastic demands computers operations research 11–12 1201–1225 doi101016s030505489900146x
rigo rizzoli soncinisessa weber zenesi 2001 neurodynamic programming efficient management reservoir networks proceedings modsim 2001 international congress modelling simulation modsim 2001 international congress modelling simulation canberra australia modelling simulation society australia zealand doi105281zenodo7481 isbn 0867405252 retrieved july 2012
damas salmeron diaz ortega prieto olivares 2000 genetic algorithms neurodynamic programming application water supply networks proceedings 2000 congress evolutionary computation 2000 congress evolutionary computation jolla california ieee doi101109cec2000870269 isbn 0780363752 retrieved july 2012
deng geng ferris 2008 neurodynamic programming fractionated radiotherapy planning springer optimization applications springer optimization applications 47–70 citeseerx 10111378288 doi10100797803877329923 isbn 9780387732985
ting learning algorithm cmac based neural processing letters 2004 4961
ting continuous cmacqrls systolic array neural processing letters 2005
forouzanfar dajani groza bolic rajan july 2010 comparison feedforward neural network training algorithms oscillometric blood pressure estimation workshop soft computing applications arad romania ieee
rigo castelletti rizzoli soncinisessa weber january 2005 selective improvement technique fastening neurodynamic programming water resources network management pavel zítek proceedings 16th ifac world congress ifacpapersonline 16th ifac world congress prague czech republic ifac doi103182200507036cz190202172 isbn 9783902661753 retrieved december 2011 maint uses authors parameter link
ferreira 2006 designing neural networks using gene expression programming abraham baets köppen nickolay applied soft computing technologies challenge complexity pages 517–536 springerverlag
xiurun july 2005 villmann improved psobased simulated annealing technique aspects neurocomputing 11th european symposium artificial neural networks elsevier doi101016jneucom200407002
chen 2009 wang shen huang zeng novel nonparametric regression ensemble rainfall forecasting using particle swarm optimization technique coupled artificial neural network international symposium neural networks isnn 2009 springer doi10100797836420151376 isbn 9783642012150 maint uses editors parameter link
ivakhnenko alexey grigorevich 1968 group method data handling rival method stochastic approximation soviet automatic control 43–55
ivakhnenko alexey 1971 polynomial theory complex systems ieee transactions systems cybernetics 364–378 doi101109tsmc19714308320
kondo ueno 2008 multilayered gmdhtype neural network selfselecting optimum neural network architecture application 3dimensional medical image recognition blood vessels international journal innovative computing information control 175–187
fukushima 1980 neocognitron selforganizing neural network model mechanism pattern recognition unaffected shift position biol cybern 193–202 doi101007bf00344251 pmid 7370364
lecun backpropagation applied handwritten code recognition neural computation 541–551 1989
yann lecun 2016 slides deep learning online
unsupervised feature learning deep learning tutorial
szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew 2014 going deeper convolutions computing research repository arxiv14094842 doi101109cvpr20157298594 isbn 9781467369640
hochreiter sepp schmidhuber jürgen 19971101 long shortterm memory neural computation 1735–1780 doi101162neco1997981735 issn 08997667
learning precise timing lstm recurrent networks download available researchgate 115–143 retrieved 20170613
bayer justin wierstra daan togelius julian schmidhuber jürgen 20090914 evolving memory cell structures sequence learning artificial neural networks icann 2009 lecture notes computer science springer berlin heidelberg 5769 755–764 doi101007978364204277576 isbn 9783642042768
fernández santiago graves alex schmidhuber jürgen 2007 sequence labelling structured domains hierarchical recurrent neural networks proc 20th joint conf artificial in℡ligence ijcai 2007 774–779
graves alex fernández santiago gomez faustino 2006 connectionist temporal classification labelling unsegmented sequence data recurrent neural networks proceedings international conference machine learning icml 2006 369–376
graves alex douglas beringer nicole schmidhuber jürgen 2003 biologically plausible speech recognition lstm neural nets intl workshop biologically inspired approaches advanced information technology bioadit 2004 lausanne switzerland 175–184
fernández santiago graves alex schmidhuber jürgen 2007 application recurrent neural networks discriminative keyword spotting proceedings 17th international conference artificial neural networks icann07 berlin heidelberg springerverlag 220–229 isbn 3540746935
hannun awni case carl casper jared catanzaro bryan diamos greg elsen erich prenger ryan satheesh sanjeev sengupta shubho 20141217 deep speech scaling endtoend speech recognition arxiv14125567 cscl
hasim senior andrew beaufays francoise 2014 long shortterm memory recurrent neural network architectures large scale acoustic modeling
xiangang xihong 20141015 constructing long shortterm memory based deep recurrent neural networks large vocabulary speech recognition arxiv14104281 cscl
qian soong 2014 synthesis bidirectional lstm based recurrent neural networks researchgate retrieved 20170613
heiga hasim 2015 unidirectional long shortterm memory recurrent neural network recurrent output layer lowlatency speech synthesis googlecom icassp 4470–4474
wang lijuan soong frank 2015 photoreal talking head deep bidirectional lstm proceedings icassp
haşim senior andrew kanishka beaufays françoise schalkwyk johan september 2015 google voice search faster accurate
gers felix schmidhuber jürgen 2001 lstm recurrent networks learn simple context free context sensitive languages ieee 1333–1340 doi10110972963769
sutskever vinyals 2014 sequence sequence learning neural networks nips14 proceedings 27th international conference neural information processing systems 3104–3112 arxiv14093215 cscl bibcode2014arxiv14093215s
jozefowicz rafal vinyals oriol schuster mike shazeer noam yonghui 20160207 exploring limits language modeling arxiv160202410 cscl
gillick brunk cliff vinyals oriol subramanya amarnag 20151130 multilingual language processing bytes arxiv151200103 cscl
vinyals oriol toshev alexander bengio samy erhan dumitru 20141117 show tell neural image caption generator arxiv14114555 cscv
gallicchio claudio micheli alessio pedrelli luca 2017 deep reservoir computing critical experimental analysis neurocomputing doi101016jneucom201612089
gallicchio claudio micheli alessio 2017 echo state property deep reservoir computing networks cognitive computation 337–350 doi101007s1255901794619 issn 18669956
hinton 2009 deep belief networks scholarpedia 5947 bibcode2009schpj45947h doi104249scholarpedia5947
larochelle hugo erhan dumitru courville aaron bergstra james bengio yoshua 2007 empirical evaluation deep architectures problems many factors variation proceedings 24th international conference machine learning icml york 473–480 doi10114512734961273556 isbn 9781595937933
graupe daniel 2013 principles artificial neural networks world scientific isbn 9789814522748
5920852 graupe large memory storage retrieval lamstar network april 1996

nigam vivek prakash graupe daniel 20040101 neuralnetworkbased detection epilepsy neurological research 55–60 doi101179016164104773026534 issn 01616412 pmid 14977058
waxman jonathan graupe daniel carley david 20100401 automated prediction apnea hypopnea using lamstar artificial neural network american journal respiratory critical care medicine 727–733 doi101164rccm2009071146oc issn 1073449x
graupe graupe zhong jackson 2008 blind adaptive filtering noninvasive extraction fetal electrocardiogram nonstationarities proc inst mech part journal engineering medicine 1221–1234 doi10124309544119jeim417
graupe 2013 240–253
graupe abon 2002 neural network blind adaptive filtering unknown noise speech intelligent engineering systems artificial neural networks technische informationsbibliothek 683–688 retrieved 20170614
graupe principles artificial neural networks3rd edition world scientific publishers 2013 pp253274
girado sandin defanti 2003 realtime camerabased face detection using modified lamstar neural network system proc spie 5015 applications artificial neural networks image processing viii applications artificial neural networks image processing viii 5015 bibcode2003spie501536g doi10111712477405
venkatachalam selvan 2007 intrusion detection using improved competitive learning lamstar network international journal computer science network security 255–263
graupe smollack 2007 control unstable nonlinear nonstationary systems using lamstar neural networks researchgate proceedings 10th iasted intelligent control sect592 141–144 retrieved 20170614
graupe daniel july 2016 deep learning neural networks design case studies world scientific publishing 57–110 isbn 9789813146471
graupe kordylewski august 1996 network based selforganizingmap modules combined statistical decision tools proceedings 39th midwest symposium circuits systems 471–474 vol1 doi101109mwscas1996594203 isbn 0780336364
graupe kordylewski 19980301 large memory storage retrieval neural network adaptive retrieval diagnosis international journal software engineering knowledge engineering 115–138 doi101142s0218194098000091 issn 02181940
kordylewski graupe 2001 novel largememory neural network medical diagnosis applications ieee transactions information technology biomedicine 202–209 doi1011094233945291
schneider graupe 2008 modified lamstar neural network applications international journal neural systems 331–337 doi101142s0129065708001634
graupe 2013
vincent pascal larochelle hugo lajoie isabelle bengio yoshua manzagol pierreantoine 2010 stacked denoising autoencoders learning useful representations deep network local denoising criterion journal machine learning research 3371–3408
ballard dana 1987 modular learning neural networks proceedings aaai 279–284
deng dong platt john 2012 scalable stacking learning building deep architectures 2012 ieee international conference acoustics speech signal processing icassp 2133–2136
deng dong 2011 deep convex scalable architecture speech pattern classification proceedings interspeech 2285–2288
david wolpert 1992 stacked generalization neural networks 241–259 doi101016s0893608005800231
bengio 20091115 learning deep architectures foundations trends® machine learning 1–127 doi1015612200000006 issn 19358237
hutchinson brian deng dong 2012 tensor deep stacking networks ieee transactions pattern analysis machine intelligence 1–15 1944–1957 doi101109tpami2012268
hinton geoffrey salakhutdinov ruslan 2006 reducing dimensionality data neural networks science 5786 504–507 bibcode2006sci313504h doi101126science1127647 pmid 16873662
dahl deng acero 2012 contextdependent pretrained deep neural networks largevocabulary speech recognition ieee transactions audio speech language processing 30–42 doi101109tasl20112134090
mohamed abdelrahman dahl george hinton geoffrey 2012 acoustic modeling using deep belief networks ieee transactions audio speech language processing 14–22 doi101109tasl20112109382
courville aaron bergstra james bengio yoshua 2011 spike slab restricted boltzmann machine jmlr workshop conference proceeding 233–241
courville aaron bergstra james bengio yoshua 2011 unsupervised models images spikeandslab rbms proceedings 28th international conference machine learning
mitchell beauchamp 1988 bayesian variable selection linear regression journal american statistical association 1023–1032 doi10108001621459198810478694
hinton osindero 2006 fast learning algorithm deep belief nets neural computation 1527–1554 doi101162neco20061871527 pmid 16764513
hinton geoffrey salakhutdinov ruslan 2009 efficient learning deep boltzmann machines 448–455
larochelle hugo bengio yoshua louradour jerdme lamblin pascal 2009 exploring strategies training deep neural networks journal machine learning research 1–40
coates adam carpenter blake 2011 text detection character recognition scene images unsupervised feature learning 440–445
honglak grosse roger 2009 convolutional deep belief networks scalable unsupervised learning hierarchical representations proceedings 26th annual international conference machine learning
yuanqing zhang tong 2010 deep coding network advances neural
ranzato marc aurelio boureau ylan 2007 sparse feature learning deep belief networks advances neural information processing systems
socher richard clif 2011 parsing natural scenes natural language recursive neural networks proceedings 26th international conference machine learning
taylor graham hinton geoffrey 2006 modeling human motion using binary latent variables advances neural information processing systems
vincent pascal larochelle hugo 2008 extracting composing robust features denoising autoencoders proceedings 25th international conference machine learning icml 1096–1103
kemp charles perfors tenenbaum joshua 2007 learning overhypotheses hierarchical bayesian models developmental science 307–21 doi101111j14677687200700585x pmid 17444972
tenenbaum joshua 2007 word learning bayesian inference psychol 245–72 doi1010370033295x1142245 pmid 17500627
chen polatkan gungor 2011 hierarchical beta process convolutional factor analysis deep learning machine learning
feifei fergus 2006 oneshot learning object categories ieee transactions pattern analysis machine intelligence 594–611 doi101109tpami200679 pmid 16566508
rodriguez abel dunson david 2008 nested dirichlet process journal american statistical association 1131–1154 doi101198016214508000000553
ruslan salakhutdinov joshua tenenbaum 2012 learning hierarchicaldeep models ieee transactions pattern analysis machine intelligence 1958–71 doi101109tpami2012269 pmid 23787346
chalasani rakesh principe jose 2013 deep predictive coding networks arxiv13013541 cslg
hinton geoffrey 1984 distributed representations
giles learning context free grammars limitations recurrent neural network external stack memory proc 14th annual conf 1992
mozer 1993 connectionist symbol manipulator discovers structure contextfree languages nips 863–870
schmidhuber 1992 learning control fastweight memories alternative recurrent nets neural computation 131–139 doi101162neco199241131
gers schraudolph schmidhuber 2002 learning precise timing lstm recurrent networks jmlr 115–143
jürgen schmidhuber 1993 introspective network learn weight change algorithm proc intl conf artificial neural networks brighton 191–195
hochreiter sepp younger steven conwell peter 2001 learning learn using gradient descent icann 2130 87–94
grefenstette edward learning transduce unbounded memoryarxiv150602516 2015
graves alex greg wayne danihelka neural turing machines arxiv14105401 2014
atkeson christopher schaal stefan 1995 memorybased neural networks robot learning neurocomputing 243–269 doi1010160925231295000336
salakhutdinov ruslan geoffrey hinton semantic hashing international journal approximate reasoning 2009 969978
quoc mikolov tomas 2014 distributed representations sentences documents arxiv14054053 cscl
weston jason sumit chopra antoine bordes memory networks arxiv14103916 2014
sukhbaatar sainbayar endtoend memory networks arxiv150308895 2015
bordes antoine largescale simple question answering memory networks arxiv150602075 2015
vinyals oriol meire fortunato navdeep jaitly pointer networks arxiv150603134 2015
kurach karol andrychowicz marcin sutskever ilya neural randomaccess machines arxiv151106392 2015
kalchbrenner blunsom 2013 recurrent continuous translation models emnlp’2013
sutskever vinyals 2014 sequence sequence learning neural networks nips’2014
merrienboer gulcehre bougares schwenk bengio october 2014 learning phrase representations using encoderdecoder statistical machine translation proceedings empiricial methods natural language processing 1406 arxiv14061078 arxiv14061078 bibcode2014arxiv14061078c
kyunghyun aaron courville yoshua bengio describing multimedia content using attentionbased encoderdecoder networks arxiv150701053 2015
scholkopf smola alexander 1998 nonlinear component analysis kernel eigenvalue problem neural computation 1299–1319 doi101162089976698300017467
youngmin 2012 kernel methods deep learning
deng gokhan xiaodong hakkanitür dilek 20121201 kernel deep convex networks endtoend learning spoken language understanding microsoft research
zissis dimitrios october 2015 cloud based architecture capable perceiving predicting multiple vessel behaviour applied soft computing 652–661 doi101016jasoc201507002
roman balabin ekaterina lomakina 2009 neural network approach quantumchemistry data accurate prediction density functional theory energies chem phys 074104 bibcode2009jchph131g4104b doi10106313206326 pmid 19708729
sengupta nandini sahidullah saha goutam august 2016 lung sound classification using cepstralbased statistical features computers biology medicine 118–129 doi101016jcompbiomed201605013
schechner 20170615 facebook boosts block terrorist propaganda wall street journal issn 00999660 retrieved 20170616
ganesan application neural networks diagnosing cancer disease using demographic data international journal computer applications
bottaci leonardo artificial neural networks applied outcome prediction colorectal cancer patients separate institutions lancet
alizadeh elaheh lyons samanthe castle jordan prasad ashok 2016 measuring systematic changes invasive cancer cell shape using zernike moments integrative biology 1183–1193 doi101039c6ib00100a pmid 27735002
lyons samanthe 2016 changes cell shape correlated metastatic potential murine biology open 289–299 doi101242bio013409
null null 20000401 artificial neural networks hydrology preliminary concepts journal hydrologic engineering 115–123 doi101061asce10840699200052115
null null 20000401 artificial neural networks hydrology hydrologic applications journal hydrologic engineering 124–137 doi101061asce10840699200052124
peres iuppa cavallaro cancelliere foti 20151001 significant wave height record extension neural networks reanalysis wind data ocean modelling 128–140 bibcode2015ocmod94128p doi101016jocemod201508002
dwarakish rakshith shetty natesan usha 2013 review applications neural network coastal engineering artificial intelligent systems machine learning 324–331
ermini leonardo catani filippo casagli nicola 20050301 artificial neural networks applied landslide susceptibility assessment geomorphology geomorphological hazard human impact mountain environments 327–343 bibcode2005geomo66327e doi101016jgeomorph200409025
introduction dynamic neural networks matlab simulink wwwmathworkscom retrieved 20170615
forrest april 2015 simulation alcohol action upon detailed purkinje neuron model simpler surrogate model runs times faster neuroscience doi101186s1286801501626
salakhutdinov ruslan hinton geoffrey 2009 semantic hashing international journal approximate reasoning 969–978 citeseerx 10111607001 doi101016jijar200811006
burgess matt deepminds learned ride london underground using humanlike reason memory wired retrieved 20161019
deepmind learns navigate london tube pcmag retrieved 20161019
mannes john deepminds differentiable neural computer helps navigate subway memory techcrunch retrieved 20161019
graves alex wayne greg reynolds malcolm harley danihelka grabskabarwińska agnieszka colmenarejo sergio gómez grefenstette edward ramalho tiago 20161012 hybrid computing using neural network dynamic external memory nature 7626 471–476 bibcode2016natur538471g doi101038nature20101 issn 14764687 pmid 27732574
differentiable neural computers deepmind deepmind retrieved 20161019
siegelmann sontag 1991 turing computability neural nets appl math lett 77–80 doi101016089396599190080f
balcázar josé 1997 computational power neural networks kolmogorov complexity characterization information theory ieee transactions 1175–1183 citeseerx 10114117782 doi10110918605580 retrieved november 2014
crick francis 1989 recent excitement neural networks nature 6203 129–132 bibcode1989natur337129c doi101038337129a0 pmid 2911347
adrian edward 1926 impulses produced sensory nerve endings journal physiology 49–72 doi101113jphysiol1926sp002273 1514809 pmid 16993776
dewdney april 1997 neutrons eyeopening tour twists turns science wiley isbn 9780471108061
felleman essen distributed hierarchical processing primate cerebral cortex cerebral cortex 1991
weng natural artificial intelligence introduction computational brainmind press isbn 9780985875725 2012
edwards chris june 2015 growing pains deep learning communications 14–16 doi1011452771283
schmidhuber jürgen 2015 deep learning neural networks overview neural networks 85–117 arxiv14047828 doi101016jneunet201409003 pmid 25462637
cade metz 2016 google built chips power bots wired
nasa dryden flight research center news room news releases nasa neural network project passes milestone nasagov retrieved 20131120
roger bridgmans defence neural networks
scaling learning algorithms towards lisa publications aigaion
bookman 1990
tahmasebi hezarkhani 2012 hybrid neural networksfuzzy logicgenetic algorithm grade estimation computers geosciences 18–27 bibcode2012cg4218t doi101016jcageo201202004


bibliographyedit

bhadeshia 1999 neural networks materials science isij international 966–979 doi102355isijinternational39966
bishop christopher 1995 neural networks pattern recognition clarendon press isbn 0198538499 oclc 33101074
cybenko 2006 approximation superpositions sigmoidal function schuppen mathematics control signals systems springer international 303–314
dewdney 1997 neutrons eyeopening tour twists turns science york wiley isbn 9780471108061 oclc 35558945
duda richard hart peter elliot stork david 2001 pattern classification wiley isbn 0471056693 oclc 41347061
egmontpetersen ridder handels 2002 image processing neural networks review pattern recognition 2279–2301 doi101016s0031320301001789
gurney kevin 1997 introduction neural networks press isbn 1857286731 oclc 37875698
haykin simon 1999 neural networks comprehensive foundation prentice hall isbn 0132733501 oclc 38908586
fahlman lebiere 1991 cascadecorrelation learning architecture created national science foundation contract number eet8716324 defense advanced research projects agency arpa order 4976 contract f3361587c1499
hertz palmer richard krogh anders 1991 introduction theory neural computation addisonwesley isbn 0201515601 oclc 21522159
lawrence jeanette 1994 introduction neural networks design theory applications california scientific software isbn 1883157005 oclc 32179420
information theory inference learning algorithms cambridge university press isbn 9780521642989 oclc 52377690
mackay david 2003 information theory inference learning algorithms cambridge university press isbn 9780521642989
masters timothy 1994 signal image processing neural networks sourcebook wiley isbn 0471049638 oclc 29877717
ripley brian 2007 pattern recognition neural networks cambridge university press isbn 9780521717700
siegelmann sontag eduardo 1994 analog computation neural networks theoretical computer science 331–360 doi1010160304397594901783
1944 smith murray 1993 neural networks statistical modeling nostrand reinhold isbn 0442013108 oclc 27145760 smith murray 1993 neural networks statistical modeling nostrand reinhold isbn 0442013108
wasserman philip 1993 advanced methods neural computing nostrand reinhold isbn 0442004613 oclc 27429729
kruse rudolf borgelt christian klawonn moewes christian steinbrecher matthias held pascal 2013 computational intelligence methodological introduction springer isbn 9781447150121 oclc 837524179
borgelt christian 2003 neurofuzzysysteme grundlagen künstlicher neuronaler netze kopplung fuzzysystemen vieweg isbn 9783528252656 oclc 76538146

external linksedit



wikibooks book topic artificial neural networks



neural networks curlie based dmoz
brief introduction neural networks illustrated 250p textbook covering common kinds neural networks license
introduction deep neural networks
tutorial neural network excel
course neural networks youtube
concise introduction machine learning artificial neural networks
neural networks machine learning course geoffrey hinton
deep learning
aplikasi pendeteksi fraud pada event proses bisnis pengadaan barang jasa menggunakan algoritma heuristic miner






retrieved httpsenwikipediaorgwindexphptitleartificialneuralnetworkoldid818555258 categories computational statisticsartificial neural networksclassification algorithmscomputational neurosciencemarket researchmarket segmentationmathematical psychologymathematical quantitative methods economicshidden categories maint explicit alcs1 maint uses editors parametercs1 germanlanguage sources decs1 maint uses authors parameteruse dates june 2013all articles unsourced statementsarticles unsourced statements august 2017wikipedia articles needing clarification april 2017all wikipedia articles needing clarificationwikipedia articles needing clarification january 2018all articles links needing disambiguationarticles links needing disambiguation december 2017wikipedia articles needing clarification june 2017articles unsourced statements june 2017articles unsourced statements february 2017articles unsourced statements november 2014articles dmoz links
