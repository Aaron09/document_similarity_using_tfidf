{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os import popen\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mypath = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first line of the following code is just syntatic sugar for a for loop.\n",
    "\n",
    "It loops throuugh every object inside the wikipedia files directory (in this case './data/') and if it is a file (rather than a directory) adds it to the array of files.\n",
    "\n",
    "Only files is now an array which holds strings that are the path to each of the wikipedia text files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carthage.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "onlyfiles[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following loop, I read each file, pre-process it (remove stopwords, lower case, etc), and then write it to a new file inside the cleaned_data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 0 of 9191\n",
      "file 1 of 9191\n",
      "file 2 of 9191\n",
      "file 3 of 9191\n",
      "file 4 of 9191\n",
      "file 5 of 9191\n",
      "file 6 of 9191\n",
      "file 7 of 9191\n",
      "file 8 of 9191\n",
      "file 9 of 9191\n",
      "file 10 of 9191\n",
      "file 11 of 9191\n",
      "file 12 of 9191\n",
      "file 13 of 9191\n",
      "file 14 of 9191\n",
      "file 15 of 9191\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-690aac1e5eb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cleaned_data/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             print(\" \".join([word for word in line.lower().translate(str.maketrans('', '', string.punctuation)).split() \n\u001b[0m\u001b[1;32m      8\u001b[0m             if len(word) >=4 and word not in stopwords.words('english')]), file=outFile)\n",
      "\u001b[0;32m<ipython-input-4-690aac1e5eb9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             print(\" \".join([word for word in line.lower().translate(str.maketrans('', '', string.punctuation)).split() \n\u001b[0;32m----> 8\u001b[0;31m             if len(word) >=4 and word not in stopwords.words('english')]), file=outFile)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mline_tokenize\u001b[0;34m(text, blanklines)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblanklines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLineTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblanklines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# If requested, strip off blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard-eof'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# If requested, strip off blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard-eof'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for f in onlyfiles:\n",
    "    print('file '+ str(i) + ' of ' + str(len(onlyfiles)))\n",
    "    i+=1\n",
    "    with open('data/'+f,'r') as inFile, open('cleaned_data/'+f,'w') as outFile:\n",
    "        for line in inFile.readlines():\n",
    "            print(\" \".join([word for word in line.lower().translate(str.maketrans('', '', string.punctuation)).split() \n",
    "            if len(word) >=4 and word not in stopwords.words('english')]), file=outFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below i import the library with the prebuilt tfidf vectorization as well as some useful utilities for speeding things up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i create the array file_paths which holds strings that are the path to each file in the cleaned_data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_paths = ['cleaned_'+mypath+f for f in onlyfiles]\n",
    "#file_dirs = [open(x).read() for x in file_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I define the tfidf vectorizer. Futher details of its parameters of it can be found at the following link:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "and its implementation can be found at:\n",
    "https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py\n",
    "\n",
    "Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification. The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less\n",
    "informative than features that occur in a small fraction of the training\n",
    "corpus.\n",
    "\n",
    "The formula that is used to compute the tf-idf of term t is tf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as idf(d, t) = log [ n / df(d, t) ] + 1 (if ``smooth_idf=False``),\n",
    "where n is the total number of documents and df(d, t) is the document frequency; the document frequency is the number of documents d that contain term t. The effect of adding \"1\" to the idf in the equation above is that terms with zero idf, i.e., terms  that occur in all documents in a training set, will not be entirely ignored.\n",
    "\n",
    "(Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(d, t) = log [ n / (df(d, t) + 1) ]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='filename',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(input='filename')\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here by calling the tfidf vectorizer's fit_transform function on all of the wikipedia files i learn the TFIDF weights for each word in the dataset. this function returns a document term matrix that has the appearences of every word in a document as well as its weight for each of these terms document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix = vectorizer.fit_transform(file_paths)\n",
    "tfidf_matrix[:10,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can see what the output of the fit tranform function looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 47109)\t0.00929048963563\n",
      "  (0, 123975)\t0.0211506061506\n",
      "  (0, 83745)\t0.0100610022266\n",
      "  (0, 121066)\t0.0105105866428\n",
      "  (0, 121741)\t0.0101203934128\n",
      "  (0, 122439)\t0.0108713865485\n",
      "  (0, 123160)\t0.00891317810767\n",
      "  (0, 119592)\t0.00875726542803\n",
      "  (0, 120418)\t0.0109716443936\n",
      "  (0, 118878)\t0.010801049572\n",
      "  (0, 120783)\t0.0107380332352\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix[1928,:124431])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now save the matrix so i dont need to recaluclate at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.sparse.save_npz('/tmp/tfidf_matrix.npz', tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also save the current weights / state of the vectorizor (this also lets it remember the vocabulary mapping it has learned. i.e. above we see that word 47109 has 0 occurences in document 1928. this raw data means nothing to us but the vectorizer keeps a mapping of words to their locations in the vector that it generates. we will need this later to map backwards to words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open(\"vectorizer.pickle\",\"wb\")\n",
    "pickle.dump(vectorizer, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now transform (convert to vector) a test document and save it to the varible X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cleaned_data/Minas Gerais.txt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x1854706 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3002 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(file_paths[3:4])\n",
    "X = vectorizer.transform(file_paths[3:4])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the following code does is analyze all 10000 wikipedia articles, for each, it converts it to a vector, pulls the mapping from each integer to its coresponding english word, sorts the words in the document by their tfidf score, takes the top n words from each, maps them to english and uses them as tags for the document.\n",
    "\n",
    "The end result is a large dictonary (hashmap) that maps each wikipedia article to a set of tags \n",
    "Bad news first:\n",
    "This code takes a truly ridiclous amount of memory to run. (if you ran it in this form due to some bad memory deallocation within the libraries it could easily take over 1000 gb of RAM).\n",
    "\n",
    "There are two pieces of good news:\n",
    "    1) I have written bash scripts (also in this directory) that chunk this problem up into ~20 equal parts that use 100 gb of ram or so a piece.\n",
    "    2) I have already computed the output of this computation using a server with ~96 gb of ram into wiki_tags.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "post_tags_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get these top terms you have to do a little bit of a song and dance to get the matrices as numpy arrays instead and in the right form.\n",
    "\n",
    "The argsort call is really the useful one, here are the docs for it: https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n",
    "\n",
    "We have to do [::-1] because  argsort only supports sorting small to large. We call flatten to reduce the dimensions to 1d so that the sorted indices can be used to index the 1d feature array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 1 of 9191\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2909\u001b[0m                 \u001b[0;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2910\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2911\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-7cc2cd14e5d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfeature_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(len(feature_array))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "feature_array = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "for (file_name, file) in zip(onlyfiles, file_paths):\n",
    "    print('file '+ str(i) + ' of ' + str(len(onlyfiles)))\n",
    "    i += 1\n",
    "    X =  vectorizer.transform([file])\n",
    "    #print(len(feature_array))\n",
    "    tfidf_sorting = np.argsort(X.toarray()).flatten()[::-1]\n",
    "    #print(type(tfidf_sorting))\n",
    "    top_n = feature_array[tfidf_sorting][:n]\n",
    "    post_tags_dict[file_name] = top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Africa.txt': '00',\n",
       " 'Cladistics.txt': 'cladistic',\n",
       " 'Economic Community of West African States.txt': '00',\n",
       " 'GLONASS.txt': '00',\n",
       " 'ISO 8000.txt': '00',\n",
       " 'Minas Gerais.txt': '00'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_tags_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open(\"wiki_article_tags.pickle\",\"wb\")\n",
    "pickle.dump(post_tags_dict, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the cosine similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the background work we are ready for a new request from the user. We take their input from the 'test.txt' file and vectorize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = vectorizer.transform(['test.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute its similairty to all the vectors representing our wikipedia articles, sort them least similar to most, and take the last k elements (these are the k most similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5261],\n",
       "       [ 616],\n",
       "       [2922],\n",
       "       [2646]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = cosine_similarity(tfidf_matrix, Y)\n",
    "file_num = np.argmax(np.array(sim))\n",
    "files = np.argsort(sim, axis=0)\n",
    "files[-k:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous operation just gave us which file number to look at from within our onlyfiles array. So we now loop through the array of number, plus them into onlyfiles and save the value stored in onlyfiles[file_number] into the file names list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oceania Football Confederation.txt',\n",
       " 'List of FIFA country codes.txt',\n",
       " 'FIFA World Cup.txt',\n",
       " 'FIFA.txt']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = [onlyfiles[num[0]] for num in files[-4:]]\n",
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now plug the the titles of each of the wikipedia articles (same as filename) into the tags hashmap to get our the associated tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fifa', '2006', 'world', 'group', 'teams', 'germany', 'june',\n",
       "       'tournament', 'goals', 'knockout'],\n",
       "      dtype='<U233')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_tags_dict[file_names[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1080i.txt': array(['1080i', 'frame', 'lines', 'interlaced', '1080', 'resolution',\n",
       "        'video', 'pixels', '1080p', 'television'],\n",
       "       dtype='<U233'),\n",
       " '10th edition of Systema Naturae.txt': array(['linnaeus', 'systema', 'naturae', 'beetles', '10th', 'species',\n",
       "        'edition', 'plantarum', 'animals', 'nomenclature'],\n",
       "       dtype='<U233'),\n",
       " '110 film.txt': array(['kodak', 'film', 'cameras', 'cartridge', 'slide', 'camera',\n",
       "        'format', 'lomography', 'slides', 'eastman'],\n",
       "       dtype='<U233'),\n",
       " '12-hour clock.txt': array(['noon', 'midnight', 'time', 'clock', '12hour', '1200', '24hour',\n",
       "        'midnightb', '1159', 'clocks'],\n",
       "       dtype='<U233'),\n",
       " '126 film.txt': array(['kodak', 'film', 'format', 'cartridge', 'cameras', 'instamatic',\n",
       "        'camera', 'ferrania', 'adox', 'eastman'],\n",
       "       dtype='<U233'),\n",
       " '135 film.txt': array(['film', 'kodak', 'camera', 'cameras', 'leica', 'format', '24',\n",
       "        'retina', 'cassettes', 'slrs'],\n",
       "       dtype='<U233'),\n",
       " '16:9.txt': array(['channels', 'ratio', 'aspect', 'television', 'channel', 'format',\n",
       "        'digital', '1771', 'freeview', 'broadcast'],\n",
       "       dtype='<U233'),\n",
       " '1790 United States Census.txt': array(['census', '1790', 'northeast', 'population', 'virginia', 'states',\n",
       "        'massachusetts', 'united', 'figures', 'vermont'],\n",
       "       dtype='<U233'),\n",
       " '1800 United States Census.txt': array(['white', 'northeast', 'free', 'females', 'census', 'males',\n",
       "        'district', '1800', '26', 'massachusetts'],\n",
       "       dtype='<U233'),\n",
       " '180th meridian.txt': array(['180000', 'meridian', '180', 'time', '180th', 'ocean', 'clock',\n",
       "        'fiji', 'tropic', '179'],\n",
       "       dtype='<U233'),\n",
       " '1810 United States Census.txt': array(['northeast', 'census', 'massachusetts', '1810', 'white', 'free',\n",
       "        'females', 'south', 'pennsylvania', 'males'],\n",
       "       dtype='<U233'),\n",
       " '1820 United States Census.txt': array(['northeast', 'persons', 'census', 'massachusetts', 'free', '2645',\n",
       "        'slaves', 'colored', 'engaged', 'males'],\n",
       "       dtype='<U233'),\n",
       " '1830 United States Census.txt': array(['northeast', 'massachusetts', 'south', 'pennsylvania', 'census',\n",
       "        'york', '1830', 'midwest', 'virginia', 'maryland'],\n",
       "       dtype='<U233'),\n",
       " '1840 United States Census.txt': array(['northeast', 'census', 'massachusetts', 'south', 'pennsylvania',\n",
       "        '1840', 'midwest', 'calhoun', 'york', 'states'],\n",
       "       dtype='<U233'),\n",
       " '1850 United States Census.txt': array(['northeast', 'census', '1850', 'massachusetts', 'south', 'midwest',\n",
       "        'total', 'pennsylvania', 'united', 'utah'],\n",
       "       dtype='<U233'),\n",
       " '1860 United States Census.txt': array(['northeast', 'census', 'massachusetts', 'midwest', '1860', 'south',\n",
       "        'population', 'york', 'jersey', 'ipums'],\n",
       "       dtype='<U233'),\n",
       " '1870 United States Census.txt': array(['northeast', 'census', '1870', 'midwest', 'population',\n",
       "        'massachusetts', 'york', 'south', 'undercount', 'pennsylvania'],\n",
       "       dtype='<U233'),\n",
       " '1880 United States Census.txt': array(['northeast', 'census', 'midwest', '1880', 'massachusetts', 'south',\n",
       "        'population', 'pennsylvania', 'bureau', 'jersey'],\n",
       "       dtype='<U233'),\n",
       " '1890 United States Census.txt': array(['census', 'northeast', 'midwest', '1890', 'massachusetts', 'bureau',\n",
       "        'south', 'population', 'hollerith', 'pennsylvania'],\n",
       "       dtype='<U233'),\n",
       " '1900 Summer Olympics.txt': array(['olympic', '1900', 'olympics', 'games', 'summer', 'events', 'paris',\n",
       "        'sports', 'croquet', 'french'],\n",
       "       dtype='<U233'),\n",
       " '1900 United States Census.txt': array(['northeast', 'midwest', 'census', 'south', 'massachusetts',\n",
       "        'pennsylvania', 'jersey', 'york', 'ohio', '1900'],\n",
       "       dtype='<U233'),\n",
       " '1908 Summer Olympics.txt': array(['1908', 'olympics', 'games', 'olympic', 'london', 'summer',\n",
       "        '20070927', 'accessed', 'club', 'wayback'],\n",
       "       dtype='<U233'),\n",
       " '1910 United States Census.txt': array(['northeast', 'midwest', 'census', '1910', 'whether', 'south',\n",
       "        'massachusetts', 'jersey', 'person', 'pennsylvania'],\n",
       "       dtype='<U233'),\n",
       " '1912 Summer Olympics.txt': array(['1913', 'official', 'report', '1912', 'olympics', 'olympic',\n",
       "        'medal', 'games', 'team', 'summer'],\n",
       "       dtype='<U233'),\n",
       " '1920 Summer Olympics.txt': array(['1920', 'olympics', 'sportsreferencecom', 'antwerp', 'olympic',\n",
       "        'summer', 'games', 'viith', 'january', 'belgian'],\n",
       "       dtype='<U233'),\n",
       " '1920 United States Census.txt': array(['northeast', 'census', 'midwest', 'south', 'massachusetts', 'ohio',\n",
       "        'jersey', 'pennsylvania', '1920', 'york'],\n",
       "       dtype='<U233'),\n",
       " '1924 Summer Olympics.txt': array(['1924', 'olympics', 'olympic', 'summer', 'games', 'stade', 'paris',\n",
       "        'french', 'wayback', '2011'],\n",
       "       dtype='<U233'),\n",
       " '1928 Summer Olympics.txt': array(['1928', 'olympics', 'olympic', 'summer', 'games', 'amsterdam',\n",
       "        'medal', 'wayback', 'sports', 'official'],\n",
       "       dtype='<U233'),\n",
       " '1930 United States Census.txt': array(['northeast', 'midwest', 'census', 'south', '1930', 'massachusetts',\n",
       "        'ohio', 'jersey', 'west', 'pennsylvania'],\n",
       "       dtype='<U233'),\n",
       " '1932 Summer Olympics.txt': array(['1932', 'olympics', 'olympic', 'summer', 'angeles', 'games',\n",
       "        'accessed', 'coliseum', 'wayback', 'stadium'],\n",
       "       dtype='<U233'),\n",
       " '1936 Summer Olympics.txt': array(['olympic', 'olympics', '1936', 'games', 'berlin', 'nazi', 'hitler',\n",
       "        'summer', 'german', 'accessed'],\n",
       "       dtype='<U233'),\n",
       " '1940 United States Census.txt': array(['midwest', 'census', 'northeast', 'south', 'west', '1940',\n",
       "        'massachusetts', 'ohio', 'population', 'jersey'],\n",
       "       dtype='<U233'),\n",
       " '1948 Summer Olympics.txt': array(['olympic', '1948', 'olympics', 'games', 'london', 'summer', 'gold',\n",
       "        'medals', 'medal', 'wembley'],\n",
       "       dtype='<U233'),\n",
       " '1950 United States Census.txt': array(['midwest', 'northeast', 'census', 'south', 'west', 'ohio', 'texas',\n",
       "        'jersey', 'bureau', '1950'],\n",
       "       dtype='<U233'),\n",
       " '1952 Summer Olympics.txt': array(['olympic', '1952', 'games', 'olympics', 'helsinki', 'summer',\n",
       "        'football', 'athletes', 'finland', 'host'],\n",
       "       dtype='<U233'),\n",
       " '1960 Summer Olympics.txt': array(['olympic', 'olympics', 'games', 'summer', '1960', 'stadio', 'rome',\n",
       "        'stadium', 'preliminaries', 'footballsoccer'],\n",
       "       dtype='<U233'),\n",
       " '1960 United States Census.txt': array(['midwest', 'south', 'northeast', 'census', 'west', 'texas',\n",
       "        'california', 'ohio', 'indiana', 'bureau'],\n",
       "       dtype='<U233'),\n",
       " '1964 Summer Olympics.txt': array(['olympics', 'tokyo', '1964', 'games', 'olympic', 'summer', 'japan',\n",
       "        'komazawa', 'football', 'city'],\n",
       "       dtype='<U233'),\n",
       " '1968 Summer Olympics.txt': array(['olympic', 'games', '1968', 'olympics', 'mexico', 'summer',\n",
       "        'estadio', 'medal', 'city', 'tlatelolco'],\n",
       "       dtype='<U233'),\n",
       " '1970 United States Census.txt': array(['midwest', 'south', 'census', 'west', 'northeast', 'california',\n",
       "        'texas', 'ohio', 'virginia', 'florida'],\n",
       "       dtype='<U233'),\n",
       " '1972 Summer Olympics.txt': array(['olympic', 'munich', 'olympics', '1972', 'games', 'summer',\n",
       "        'messegelände', 'germany', 'preliminaries', 'massacre'],\n",
       "       dtype='<U233'),\n",
       " '1973 oil crisis.txt': array(['embargo', 'opec', 'price', 'energy', '1973', 'crisis', 'petroleum',\n",
       "        '1974', 'arab', 'saudi'],\n",
       "       dtype='<U233'),\n",
       " '1976 Summer Olympics.txt': array(['olympic', 'montreal', 'olympics', 'games', 'canada', '1976',\n",
       "        'quebec', 'stadium', 'summer', 'canadian'],\n",
       "       dtype='<U233'),\n",
       " '1980 Summer Olympics.txt': array(['olympics', 'olympic', 'games', 'gold', 'summer', 'soviet', 'ussr',\n",
       "        '1980', 'moscow', 'medal'],\n",
       "       dtype='<U233'),\n",
       " '1980 United States Census.txt': array(['census', 'midwest', 'south', 'west', 'california', 'northeast',\n",
       "        'texas', 'bureau', 'ohio', 'york'],\n",
       "       dtype='<U233'),\n",
       " '1984 Summer Olympics.txt': array(['olympics', 'olympic', 'games', '1984', 'angeles', 'summer',\n",
       "        'california', 'boycott', 'medal', 'stadium'],\n",
       "       dtype='<U233'),\n",
       " '1988 Summer Olympics.txt': array(['olympic', 'olympics', 'seoul', '1988', 'games', 'summer', '2007',\n",
       "        'korea', 'october', 'demonstration'],\n",
       "       dtype='<U233'),\n",
       " '1990 United States Census.txt': array(['census', 'south', 'midwest', 'west', 'california', '1990', 'texas',\n",
       "        'bureau', 'seats', 'states'],\n",
       "       dtype='<U233'),\n",
       " '1992 Summer Olympics.txt': array(['olympic', '1992', 'barcelona', 'olympics', 'summer', 'pavelló',\n",
       "        'games', 'radio', 'olímpic', 'medal'],\n",
       "       dtype='<U233'),\n",
       " '1996 Summer Olympics.txt': array(['olympic', 'olympics', 'games', 'atlanta', 'summer', 'georgia',\n",
       "        '1996', 'centennial', 'stadium', 'medal'],\n",
       "       dtype='<U233'),\n",
       " '1997 Asian financial crisis.txt': array(['crisis', 'financial', 'asian', 'currency', 'bubble', 'dollar',\n",
       "        'exchange', 'stock', '1997', 'economic'],\n",
       "       dtype='<U233'),\n",
       " '2000 Summer Olympics.txt': array(['olympic', 'sydney', 'games', 'olympics', '2000', 'summer',\n",
       "        'centre', 'stadium', 'australian', 'ceremony'],\n",
       "       dtype='<U233'),\n",
       " '2000 United States Census.txt': array(['census', 'americans', 'south', 'population', 'samesex', 'west',\n",
       "        'midwest', '2000', 'states', 'utah'],\n",
       "       dtype='<U233'),\n",
       " '2002 Winter Olympics.txt': array(['olympic', 'winter', 'games', 'olympics', '2002', 'lake', 'salt',\n",
       "        'utah', '0971796106', 'skiing'],\n",
       "       dtype='<U233'),\n",
       " '2003 invasion of Iraq.txt': array(['iraqi', 'iraq', 'invasion', 'saddam', 'forces', '2003', 'baghdad',\n",
       "        'bush', 'coalition', 'fedayeen'],\n",
       "       dtype='<U233'),\n",
       " '2004 Indian Ocean earthquake and tsunami.txt': array(['tsunami', 'earthquake', 'sumatra', 'waves', 'ocean', 'indian',\n",
       "        'andaman', 'aceh', 'wave', 'coast'],\n",
       "       dtype='<U233'),\n",
       " '2004 Summer Olympics.txt': array(['olympic', 'athens', 'games', 'olympics', '2004', 'stadium',\n",
       "        'greek', 'march', 'centre', '2010'],\n",
       "       dtype='<U233'),\n",
       " '2006 FIFA World Cup.txt': array(['fifa', '2006', 'world', 'group', 'teams', 'germany', 'june',\n",
       "        'tournament', 'goals', 'knockout'],\n",
       "       dtype='<U233'),\n",
       " '2006 Winter Olympics.txt': array(['olympic', 'games', 'winter', 'olympics', 'turin', '2006', 'skiing',\n",
       "        'skating', 'february', 'torino'],\n",
       "       dtype='<U233'),\n",
       " '2008 Summer Olympics.txt': array(['olympic', '2008', 'beijing', 'games', 'olympics', 'august',\n",
       "        'retrieved', 'china', 'summer', '2009'],\n",
       "       dtype='<U233'),\n",
       " '2010 FIFA World Cup.txt': array(['fifa', '2010', 'world', 'stadium', 'june', 'johannesburg', 'south',\n",
       "        'cape', 'group', 'july'],\n",
       "       dtype='<U233'),\n",
       " '2010 United States Census.txt': array(['census', 'statistical', '2010', 'metropolitan', 'area', 'bureau',\n",
       "        'south', 'west', 'population', 'combined'],\n",
       "       dtype='<U233'),\n",
       " '2010 Winter Olympics.txt': array(['vancouver', '2010', 'olympic', 'olympics', 'winter', 'games',\n",
       "        'february', 'canada', 'retrieved', 'whistler'],\n",
       "       dtype='<U233'),\n",
       " '2012 Summer Olympics.txt': array(['2012', 'olympic', 'london', 'olympics', 'games', 'july',\n",
       "        'retrieved', '2011', 'august', 'summer'],\n",
       "       dtype='<U233'),\n",
       " '2016 Summer Olympics.txt': array(['olympic', '2016', 'olympics', 'games', 'summer', 'athletes',\n",
       "        'retrieved', 'august', 'casa', 'ceremony'],\n",
       "       dtype='<U233'),\n",
       " '20th Century Fox.txt': array(['pictures', 'century', '20th', 'film', 'films', 'studios',\n",
       "        'entertainment', 'zanuck', 'cinemascope', 'star'],\n",
       "       dtype='<U233'),\n",
       " '24-hour clock.txt': array(['time', '24hour', 'clock', 'midnight', 'notation', '12hour', '2400',\n",
       "        '0000', 'clocks', 'hour'],\n",
       "       dtype='<U233'),\n",
       " '300 (number).txt': array(['primes', 'oeis', 'sloanes', 'integer', 'sequences', '20160522',\n",
       "        'number', 'consecutive', 'encyclopedia', 'foundation'],\n",
       "       dtype='<U233'),\n",
       " '3D computer graphics.txt': array(['graphics', 'computer', 'animation', 'rendering', 'software',\n",
       "        'modeling', 'models', 'model', 'computeraided', 'autodesk'],\n",
       "       dtype='<U233'),\n",
       " '400 (number).txt': array(['20160610', 'integer', 'sloanes', 'oeis', 'sequences', 'primes',\n",
       "        'code', 'prime', 'number', 'encyclopedia'],\n",
       "       dtype='<U233'),\n",
       " '43 Things.txt': array(['amazon', 'things', 'networking', '43thingscom', 'coop', 'kindle',\n",
       "        'robot', 'goals', 'alexa', 'site'],\n",
       "       dtype='<U233'),\n",
       " '480i.txt': array(['ntsc', 'lines', '480i', 'video', 'field', 'line', 'resolution',\n",
       "        'system', 'frame', 'television'],\n",
       "       dtype='<U233'),\n",
       " '500 (number).txt': array(['bases', 'harshad', 'palindromic', 'primes', 'number', '20160611',\n",
       "        'sloanes', 'oeis', 'integer', 'sequences'],\n",
       "       dtype='<U233'),\n",
       " '501(c) organization.txt': array(['organizations', 'revenue', 'internal', '501c4', 'organization',\n",
       "        'exempt', 'service', '501c3', 'associations', '501c6'],\n",
       "       dtype='<U233'),\n",
       " '576i.txt': array(['frame', '576i', 'frames', 'ntsc', 'video', 'digital', 'baseband',\n",
       "        'lines', 'speedup', 'audio'],\n",
       "       dtype='<U233'),\n",
       " '600 (number).txt': array(['20160611', 'sloanes', 'oeis', 'integer', 'primes', 'sequences',\n",
       "        'number', 'nontotient', 'prime', 'encyclopedia'],\n",
       "       dtype='<U233'),\n",
       " '700 (number).txt': array(['primes', '20160611', 'oeis', 'sloanes', 'integer', 'sequences',\n",
       "        'number', 'nontotient', 'prime', 'consecutive'],\n",
       "       dtype='<U233'),\n",
       " '720p.txt': array(['720p', 'hdtv', 'frames', 'atsc', '720', 'resolution',\n",
       "        'progressive', 'system', 'formats', 'television'],\n",
       "       dtype='<U233'),\n",
       " '800 (number).txt': array(['20160611', 'primes', 'oeis', 'integer', 'sloanes', 'prime',\n",
       "        'sequences', 'nontotient', 'number', 'mertens'],\n",
       "       dtype='<U233'),\n",
       " '900 (number).txt': array(['20160611', 'sloanes', 'oeis', 'integer', 'sequences', 'primes',\n",
       "        'identifier', 'nontotient', 'published', 'books'],\n",
       "       dtype='<U233'),\n",
       " '90th meridian east.txt': array(['90000', '90', 'meridian', 'ocean', '90th', 'tropic', 'circle',\n",
       "        'arctic', 'coordinates', 'antarctic'],\n",
       "       dtype='<U233'),\n",
       " '90th meridian west.txt': array(['90000', '90', 'meridian', 'nunavut', 'canada', 'ocean', 'tropic',\n",
       "        'island', '45x90', 'coordinates'],\n",
       "       dtype='<U233'),\n",
       " 'A cappella.txt': array(['cappella', 'instruments', 'worship', 'music', 'barbershop',\n",
       "        'musical', 'harmony', 'groups', 'vocal', 'singing'],\n",
       "       dtype='<U233'),\n",
       " 'A.C. Milan.txt': array(['milan', 'serie', 'club', 'italian', 'madrid', '2017', 'clubs',\n",
       "        'football', 'bilancio', 'milans'],\n",
       "       dtype='<U233'),\n",
       " 'A.txt': array(['alphabet', 'letter', 'vowel', 'phonetic', 'unrounded', 'latin',\n",
       "        'alpha', 'used', 'cursive', 'script'],\n",
       "       dtype='<U233'),\n",
       " 'A440 (pitch standard).txt': array(['pitch', 'a440', 'tuning', 'standardization', 'frequency',\n",
       "        'notation', 'musical', 'piano', 'standard', 'standards'],\n",
       "       dtype='<U233'),\n",
       " 'A9.com.txt': array(['amazon', 'search', 'a9com', 'amazoncom', 'november', 'cloudsearch',\n",
       "        'advertising', 'clickriver', '2016', 'engine'],\n",
       "       dtype='<U233'),\n",
       " 'ABC News.txt': array(['news', 'correspondent', 'coanchor', 'disney', 'anchor', 'morning',\n",
       "        'chief', 'nightline', '2017present', 'america'],\n",
       "       dtype='<U233'),\n",
       " 'Abbasid Caliphate.txt': array(['abbasid', 'abbasids', 'caliphate', 'islamic', 'baghdad', 'dynasty',\n",
       "        'islam', 'caliphs', 'shaikh', 'caliph'],\n",
       "       dtype='<U233'),\n",
       " 'Abbot.txt': array(['abbot', 'abbots', 'monks', 'bishop', 'monastery', 'church',\n",
       "        'monasteries', 'abbey', 'monastic', 'title'],\n",
       "       dtype='<U233'),\n",
       " 'Abbreviation.txt': array(['abbreviations', 'abbreviation', 'syllabic', 'contraction',\n",
       "        'periods', 'letters', 'style', 'plural', 'letter', 'acronyms'],\n",
       "       dtype='<U233'),\n",
       " 'Abdomen.txt': array(['abdomen', 'abdominal', 'organs', 'ligament', 'muscles', 'spine',\n",
       "        'rectus', 'cavity', 'thorax', 'fossa'],\n",
       "       dtype='<U233'),\n",
       " 'AbeBooks.txt': array(['abebooks', 'amazon', 'books', 'booksellers', 'amazoncom',\n",
       "        'sellers', 'marketplace', 'book', 'kindle', 'bookstores'],\n",
       "       dtype='<U233'),\n",
       " 'Aberdeen.txt': array(['aberdeen', 'city', 'scotland', 'scottish', 'aberdeens', 'street',\n",
       "        'council', 'granite', 'retrieved', '2007'],\n",
       "       dtype='<U233'),\n",
       " 'Abiogenesis.txt': array(['life', 'issn', 'molecules', 'pmid', 'earth', 'organic', 'chemical',\n",
       "        'origin', '2015', 'chemistry'],\n",
       "       dtype='<U233'),\n",
       " 'Abkhazia.txt': array(['abkhazia', 'abkhaz', 'georgian', 'russian', 'georgia', 'abkhazian',\n",
       "        'ethnic', 'georgians', 'passports', 'ossetia'],\n",
       "       dtype='<U233'),\n",
       " 'Abolitionism.txt': array(['slavery', 'slave', 'abolition', 'antislavery', 'slaves', 'trade',\n",
       "        'abolitionist', 'abolished', 'emancipation', 'trafficking'],\n",
       "       dtype='<U233'),\n",
       " 'Aboriginal Australians.txt': array(['aboriginal', 'australians', 'indigenous', 'australian', 'torres',\n",
       "        'australia', 'strait', 'islander', 'aborigines', 'people'],\n",
       "       dtype='<U233'),\n",
       " 'Aboriginal peoples in Canada.txt': array(['aboriginal', 'inuit', 'canada', 'indigenous', 'peoples', 'indian',\n",
       "        'métis', 'canadian', '2009', 'october'],\n",
       "       dtype='<U233'),\n",
       " 'Abortion.txt': array(['abortion', 'abortions', 'induced', 'pregnancy', 'health', 'unsafe',\n",
       "        'pmid', 'medical', 'obstetrics', 'fetal'],\n",
       "       dtype='<U233'),\n",
       " 'About.com.txt': array(['aboutcom', 'vogel', '2014', 'aboutcoms', 'dotdash', 'acquisition',\n",
       "        'abangcom', 'abouts', 'retrieved', 'techcrunch'],\n",
       "       dtype='<U233'),\n",
       " 'Above mean sea level.txt': array(['level', 'metres', 'altitude', 'elevation', 'mean', 'measurement',\n",
       "        'mamsl', 'abbreviations', 'merged', '20070506'],\n",
       "       dtype='<U233'),\n",
       " 'Abraham Lincoln.txt': array(['lincoln', 'abraham', 'lincolns', 'donald', 'slavery', 'republican',\n",
       "        'confederate', '1864', '1865', '1861'],\n",
       "       dtype='<U233'),\n",
       " 'Abrahamic religions.txt': array(['abrahamic', 'religions', 'islam', 'judaism', 'muslims',\n",
       "        'christianity', 'abraham', 'faith', 'christian', 'religion'],\n",
       "       dtype='<U233'),\n",
       " 'Absolute magnitude.txt': array(['magnitude', 'absolute', 'bolometric', 'luminosity', 'magnitudes',\n",
       "        'star', 'apparent', 'stars', 'displaystyle', 'mbol'],\n",
       "       dtype='<U233'),\n",
       " 'Absolute monarchy.txt': array(['absolute', 'monarchy', 'absolutism', 'power', 'monarchs',\n",
       "        'monarch', 'hereditary', 'monarchies', 'democracy', 'revolution'],\n",
       "       dtype='<U233'),\n",
       " 'Abstract (summary).txt': array(['abstract', 'abstracts', 'graphical', 'academic', 'scientific',\n",
       "        'paper', 'journal', 'informative', 'calves', 'research'],\n",
       "       dtype='<U233'),\n",
       " 'Abstract algebra.txt': array(['algebra', 'algebraic', 'theory', 'abstract', 'mathematics',\n",
       "        'equations', 'groups', 'group', 'structures', 'permutation'],\n",
       "       dtype='<U233'),\n",
       " 'Abu Dhabi.txt': array(['dhabi', 'zayed', 'sheikh', 'emirates', 'city', 'arab', 'nahyan',\n",
       "        'tower', 'emirate', 'etihad'],\n",
       "       dtype='<U233'),\n",
       " 'Academia.txt': array(['academy', 'academies', 'school', 'accademia', 'founded',\n",
       "        'sciences', 'akademia', 'education', 'schools', 'académie'],\n",
       "       dtype='<U233'),\n",
       " 'Academic Press.txt': array(['academic', 'elsevier', 'publishing', 'reed', 'press', 'lexisnexis',\n",
       "        'stub', 'neuroscience', 'expo', 'comic'],\n",
       "       dtype='<U233'),\n",
       " 'Academic degree.txt': array(['degree', 'degrees', 'education', 'bachelors', 'universities',\n",
       "        'qualifications', 'doctorate', 'academic', 'master', 'bachelor'],\n",
       "       dtype='<U233'),\n",
       " 'Academic journal.txt': array(['journals', 'academic', 'journal', 'review', 'research', 'peer',\n",
       "        'book', 'science', 'scholarly', 'articles'],\n",
       "       dtype='<U233'),\n",
       " 'Academic publishing.txt': array(['journals', 'academic', 'publishing', 'peer', 'scientific',\n",
       "        'journal', 'access', 'scholarly', 'open', 'publication'],\n",
       "       dtype='<U233'),\n",
       " 'Academy Award for Best Foreign Language Film.txt': array(['academy', 'film', 'award', 'foreign', 'awards', 'language',\n",
       "        'films', 'best', 'oscar', 'picture'],\n",
       "       dtype='<U233'),\n",
       " 'Academy Award.txt': array(['awards', 'academy', 'oscar', 'award', 'oscars', 'best', 'film',\n",
       "        'picture', 'original', 'archived'],\n",
       "       dtype='<U233'),\n",
       " 'Academy Awards.txt': array(['awards', 'academy', 'oscar', 'award', 'oscars', 'best', 'film',\n",
       "        'picture', 'original', 'archived'],\n",
       "       dtype='<U233'),\n",
       " 'Académie française.txt': array(['académie', 'française', 'lacadémie', 'dictionnaire', 'french',\n",
       "        'members', 'prizes', 'france', 'language', 'elected'],\n",
       "       dtype='<U233'),\n",
       " 'Acceleration.txt': array(['acceleration', 'velocity', 'mathbf', 'displaystyle', 'motion',\n",
       "        'vector', 'angular', 'speed', 'circular', 'centripetal'],\n",
       "       dtype='<U233'),\n",
       " 'Accountancy.txt': array(['accounting', 'financial', 'accountancy', 'accountants', 'auditing',\n",
       "        'management', 'business', 'standards', 'aicpa', '2013'],\n",
       "       dtype='<U233'),\n",
       " 'Accountant.txt': array(['accountants', 'chartered', 'accountant', 'certified',\n",
       "        'accountancy', 'accounting', 'designatory', 'institute', 'public',\n",
       "        'management'],\n",
       "       dtype='<U233'),\n",
       " 'Accounting period.txt': array(['fiscal', 'accounting', 'year', 'august', 'financial', 'leap',\n",
       "        'period', 'saturday', 'month', '53week'],\n",
       "       dtype='<U233'),\n",
       " 'Accra.txt': array(['accra', 'ghana', 'accras', 'nkrumah', 'districts', 'city', 'kwame',\n",
       "        'district', 'school', 'central'],\n",
       "       dtype='<U233'),\n",
       " 'Accusative case.txt': array(['accusative', 'nominative', 'case', 'prepositions', 'object',\n",
       "        'pronouns', 'latin', 'nouns', 'masculine', 'german'],\n",
       "       dtype='<U233'),\n",
       " 'Acetic acid.txt': array(['acetic', 'acid', 'acetate', 'vinegar', 'chemical', 'hydrogen',\n",
       "        'oxidation', 'organic', 'ethylene', 'methanol'],\n",
       "       dtype='<U233'),\n",
       " 'Achaemenid Empire.txt': array(['persian', 'darius', 'artaxerxes', 'achaemenid', 'empire', 'cyrus',\n",
       "        'herodotus', 'persia', 'persians', 'dynasty'],\n",
       "       dtype='<U233'),\n",
       " 'Acid.txt': array(['acid', 'acids', 'proton', 'hydrogen', 'arrhenius', 'brønsted',\n",
       "        'reactions', 'hydronium', 'base', 'carboxylic'],\n",
       "       dtype='<U233'),\n",
       " 'Acoustic guitar.txt': array(['guitar', 'guitars', 'acoustic', 'sound', 'strings', 'body',\n",
       "        'string', 'soundboard', 'pickups', 'parlor'],\n",
       "       dtype='<U233'),\n",
       " 'Acoustics.txt': array(['acoustics', 'acoustic', 'sound', 'wave', 'vibration', 'acoustical',\n",
       "        'noise', 'waves', 'underwater', 'propagation'],\n",
       "       dtype='<U233'),\n",
       " 'Acre.txt': array(['acre', 'square', 'yards', 'acres', 'feet', 'units', 'land', 'mile',\n",
       "        'survey', 'metres'],\n",
       "       dtype='<U233'),\n",
       " 'Acronym.txt': array(['acronym', 'acronyms', 'letters', 'word', 'pronounced', 'words',\n",
       "        'abbreviations', 'abbreviation', 'initialisms', 'stands'],\n",
       "       dtype='<U233'),\n",
       " 'Act of Congress.txt': array(['congress', 'senate', 'congressional', 'caucus', 'veto', 'house',\n",
       "        'capitol', 'bill', 'promulgation', 'united'],\n",
       "       dtype='<U233'),\n",
       " 'Act of Parliament.txt': array(['bill', 'stage', 'house', 'bills', 'amendments', 'committee',\n",
       "        'assent', 'parliament', 'senate', 'reading'],\n",
       "       dtype='<U233'),\n",
       " 'Actinopterygii.txt': array(['order', 'fish', 'sensu', 'betancurrodriguez', 'rayfinned',\n",
       "        'fishes', 'actinopterygii', 'berg', 'bony', 'neopterygii'],\n",
       "       dtype='<U233'),\n",
       " 'Action film.txt': array(['action', 'films', 'film', 'genre', 'martial', 'cinema', 'comedy',\n",
       "        'actors', 'subgenre', 'movies'],\n",
       "       dtype='<U233'),\n",
       " 'Action game.txt': array(['games', 'game', 'action', 'player', 'enemies', 'shooter', 'avatar',\n",
       "        'players', 'levels', 'firstperson'],\n",
       "       dtype='<U233'),\n",
       " 'Activism.txt': array(['activism', 'activists', 'activist', 'social', 'advocacy',\n",
       "        'environmental', 'organizations', 'political', 'lobbying', 'design'],\n",
       "       dtype='<U233'),\n",
       " 'Actor.txt': array(['actors', 'actor', 'acting', 'theatre', 'actress', 'stage', 'film',\n",
       "        'drama', 'radio', 'roles'],\n",
       "       dtype='<U233'),\n",
       " 'Actress.txt': array(['actors', 'actor', 'acting', 'theatre', 'actress', 'stage', 'film',\n",
       "        'drama', 'radio', 'roles'],\n",
       "       dtype='<U233'),\n",
       " 'Acts of Union 1707.txt': array(['scotland', 'union', 'parliament', '1707', 'england', 'scottish',\n",
       "        'earl', 'party', 'court', 'squadrone'],\n",
       "       dtype='<U233'),\n",
       " 'Acts of the Apostles.txt': array(['acts', 'luke', 'apostles', 'jesus', 'gospel', 'jerusalem',\n",
       "        'testament', 'jews', 'paul', 'church'],\n",
       "       dtype='<U233'),\n",
       " 'Actuarial science.txt': array(['actuarial', 'insurance', 'actuaries', '20060628', 'models',\n",
       "        'science', 'pension', 'actuary', 'financial', 'regression'],\n",
       "       dtype='<U233'),\n",
       " 'Ada (programming language).txt': array(['programming', 'language', 'pascal', 'task', 'compiler', 'loop',\n",
       "        'protected', 'runway', 'addisonwesley', 'type'],\n",
       "       dtype='<U233'),\n",
       " 'Adam Smith.txt': array(['smith', 'adam', 'smiths', 'wealth', 'economics', 'liberalism',\n",
       "        'moral', 'theory', 'liberal', 'labour'],\n",
       "       dtype='<U233')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_tags_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
