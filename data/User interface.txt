For the boundary between computer systems, see Interface (computing). For other uses, see Interface.




Example of a tangible user interface


The user interface (UI), in the industrial design field of human–computer interaction, is the space where interactions between humans and machines occur. The goal of this interaction is to allow effective operation and control of the machine from the human end, whilst the machine simultaneously feeds back information that aids the operators' decision-making process. Examples of this broad concept of user interfaces include the interactive aspects of computer operating systems, hand tools, heavy machinery operator controls, and process controls. The design considerations applicable when creating user interfaces are related to or involve such disciplines as ergonomics and psychology.
Generally, the goal of user interface design is to produce a user interface which makes it easy (self-explanatory), efficient, and enjoyable (user-friendly) to operate a machine in the way which produces the desired result. This generally means that the operator needs to provide minimal input to achieve the desired output, and also that the machine minimizes undesired outputs to the human.
With the increased use of personal computers and the relative decline in societal awareness of heavy machinery,[clarification needed] the term user interface is generally assumed to mean the graphical user interface, while industrial control panel and machinery control design discussions more commonly refer to human-machine interfaces.
Other terms for user interface are man–machine interface (MMI) and when the machine in question is a computer human–computer interface.



Contents


1 Overview
2 Terminology
3 History

3.1 1945–1968: Batch interface
3.2 1969–present: Command-line user interface
3.3 1985: SAA User Interface or Text-Based User Interface
3.4 1968–present: Graphical User Interface


4 Interface design

4.1 Quality
4.2 Principle of least astonishment
4.3 Habit formation


5 Types
6 Gallery
7 See also
8 References
9 External links



Overview[edit]




A graphical user interface following the desktop metaphor


The user interface or human–machine interface is the part of the machine that handles the human–machine interaction. Membrane switches, rubber keypads and touchscreens are examples of the physical part of the Human Machine Interface which we can see and touch.
In complex systems, the human–machine interface is typically computerized. The term human–computer interface refers to this kind of system. In the context of computing the term typically extends as well to the software dedicated to control the physical elements used for human-computer interaction.
The engineering of the human–machine interfaces is enhanced by considering ergonomics (human factors). The corresponding disciplines are human factors engineering (HFE) and usability engineering (UE), which is part of systems engineering.
Tools used for incorporating human factors in the interface design are developed based on knowledge of computer science, such as computer graphics, operating systems, programming languages. Nowadays, we use the expression graphical user interface for human–machine interface on computers, as nearly all of them are now using graphics.
Terminology[edit]




A human–machine interface usually involves peripheral hardware for the INPUT and for the OUTPUT. Often, there is an additional component implemented in software, like e.g. a graphical user interface.


There is a difference between a user interface and an operator interface or a human–machine interface (HMI).

The term "user interface" is often used in the context of (personal) computer systems and electronic devices

Where a network of equipment or computers are interlinked through an MES (Manufacturing Execution System)-or Host to display information.
A human-machine interface (HMI) is typically local to one machine or piece of equipment, and is the interface method between the human and the equipment/machine. An operator interface is the interface method by which multiple equipment that are linked by a host control system is accessed or controlled.[clarification needed]
The system may expose several user interfaces to serve different kinds of users. For example, a computerized library database might provide two user interfaces, one for library patrons (limited set of functions, optimized for ease of use) and the other for library personnel (wide set of functions, optimized for efficiency).[clarification needed]


The user interface of a mechanical system, a vehicle or an industrial installation is sometimes referred to as the human–machine interface (HMI).[1] HMI is a modification of the original term MMI (man-machine interface).[2] In practice, the abbreviation MMI is still frequently used[2] although some[who?] may claim that MMI stands for something different now. Another abbreviation is HCI, but is more commonly used for human–computer interaction.[2] Other terms used are operator interface console (OIC) and operator interface terminal (OIT).[3] However it is abbreviated, the terms refer to the 'layer' that separates a human that is operating a machine from the machine itself.[2] Without a clean and usable interface, humans would not be able to interact with information systems.

In science fiction, HMI is sometimes used to refer to what is better described as direct neural interface. However, this latter usage is seeing increasing application in the real-life use of (medical) prostheses—the artificial extension that replaces a missing body part (e.g., cochlear implants).[4][5]
In some circumstances, computers might observe the user and react according to their actions without specific commands. A means of tracking parts of the body is required, and sensors noting the position of the head, direction of gaze and so on have been used experimentally. This is particularly relevant to immersive interfaces.[6][7]
History[edit]
The history of user interfaces can be divided into the following phases according to the dominant type of user interface:
1945–1968: Batch interface[edit]




IBM 029


In the batch era, computing power was extremely scarce and expensive. User interfaces were rudimentary. Users had to accommodate computers rather than the other way around; user interfaces were considered overhead, and software was designed to keep the processor at maximum utilization with as little overhead as possible.
The input side of the user interfaces for batch machines were mainly punched cards or equivalent media like paper tape. The output side added line printers to these media. With the limited exception of the system operator's console, human beings did not interact with batch machines in real time at all.
Submitting a job to a batch machine involved, first, preparing a deck of punched cards describing a program and a dataset. Punching the program cards wasn't done on the computer itself, but on keypunches, specialized typewriter-like machines that were notoriously bulky, unforgiving, and prone to mechanical failure. The software interface was similarly unforgiving, with very strict syntaxes meant to be parsed by the smallest possible compilers and interpreters.




Holes are punched in the card according to a prearranged code transferring the facts from the census questionnaire into statistics


Once the cards were punched, one would drop them in a job queue and wait. Eventually. operators would feed the deck to the computer, perhaps mounting magnetic tapes to supply another dataset or helper software. The job would generate a printout, containing final results or (all too often) an abort notice with an attached error log. Successful runs might also write a result on magnetic tape or generate some data cards to be used in later computation.
The turnaround time for a single job often spanned entire days. If one were very lucky, it might be hours; real-time response was unheard of. But there were worse fates than the card queue; some computers actually required an even more tedious and error-prone process of toggling in programs in binary code using console switches. The very earliest machines actually had to be partly rewired to incorporate program logic into themselves, using devices known as plugboards.
Early batch systems gave the currently running job the entire computer; program decks and tapes had to include what we would now think of as operating system code to talk to I/O devices and do whatever other housekeeping was needed. Midway through the batch period, after 1957, various groups began to experiment with so-called “load-and-go” systems. These used a monitor program which was always resident on the computer. Programs could call the monitor for services. Another function of the monitor was to do better error checking on submitted jobs, catching errors earlier and more intelligently and generating more useful feedback to the users. Thus, monitors represented a first step towards both operating systems and explicitly designed user interfaces.
1969–present: Command-line user interface[edit]
Main article: Command-line interface




Teletype Model 33 ASR


Command-line interfaces (CLIs) evolved from batch monitors connected to the system console. Their interaction model was a series of request-response transactions, with requests expressed as textual commands in a specialized vocabulary. Latency was far lower than for batch systems, dropping from days or hours to seconds. Accordingly, command-line systems allowed the user to change his or her mind about later stages of the transaction in response to real-time or near-real-time feedback on earlier results. Software could be exploratory and interactive in ways not possible before. But these interfaces still placed a relatively heavy mnemonic load on the user, requiring a serious investment of effort and learning time to master.[8]
The earliest command-line systems combined teleprinters with computers, adapting a mature technology that had proven effective for mediating the transfer of information over wires between human beings. Teleprinters had originally been invented as devices for automatic telegraph transmission and reception; they had a history going back to 1902 and had already become well-established in newsrooms and elsewhere by 1920. In reusing them, economy was certainly a consideration, but psychology and the Rule of Least Surprise mattered as well; teleprinters provided a point of interface with the system that was familiar to many engineers and users.




DEC VT100 terminal


The widespread adoption of video-display terminals (VDTs) in the mid-1970s ushered in the second phase of command-line systems. These cut latency further, because characters could be thrown on the phosphor dots of a screen more quickly than a printer head or carriage can move. They helped quell conservative resistance to interactive programming by cutting ink and paper consumables out of the cost picture, and were to the first TV generation of the late 1950s and 60s even more iconic and comfortable than teleprinters had been to the computer pioneers of the 1940s.
Just as importantly, the existence of an accessible screen — a two-dimensional display of text that could be rapidly and reversibly modified — made it economical for software designers to deploy interfaces that could be described as visual rather than textual. The pioneering applications of this kind were computer games and text editors; close descendants of some of the earliest specimens, such as rogue(6), and vi(1), are still a live part of Unix tradition.
1985: SAA User Interface or Text-Based User Interface[edit]
In 1985, with the beginning of Microsoft Windows and other graphical user interfaces, IBM created what is called the Systems Application Architecture (SAA) standard which include the Common User Access (CUA) derivative. CUA successfully created what we know and use today in Windows, and most of the more recent DOS or Windows Console Applications will use that standard as well.
This defined that a pulldown menu system should be at the top of the screen, status bar at the bottom, shortcut keys should stay the same for all common functionality (F2 to Open for example would work in all applications that followed the SAA standard). This greatly helped the speed at which users could learn an application so it caught on quick and became an industry standard.[9]
1968–present: Graphical User Interface[edit]




AMX Desk made a basic WIMP GUI






Linotype WYSIWYG 2000, 1989



1968 – Douglas Engelbart demonstrated NLS, a system which uses a mouse, pointers, hypertext, and multiple windows.[10]
1970 – Researchers at Xerox Palo Alto Research Center (many from SRI) develop WIMP paradigm (Windows, Icons, Menus, Pointers)[10]
1973 – Xerox Alto: commercial failure due to expense, poor user interface, and lack of programs[10]
1979 – Steve Jobs and other Apple engineers visit Xerox. Pirates of Silicon Valley dramatizes the events, but Apple had already been working on the GUI before the visit
1981 – Xerox Star: focus on WYSIWYG. Commercial failure (25K sold) due to cost ($16K each), performance (minutes to save a file, couple of hours to recover from crash), and poor marketing
1984 – Apple Macintosh popularizes the GUI. Super Bowl commercial shown once, most expensive ever made at that time
1984 – MIT's X Window System: hardware-independent platform and networking protocol for developing GUIs on UNIX-like systems
1985 – Windows 1.0 – provided GUI interface to MS-DOS. No overlapping windows (tiled instead).
1985 – Microsoft and IBM start work on OS/2 meant to eventually replace MS-DOS and Windows
1986 – Apple threatens to sue Digital Research because their GUI desktop looked too much like Apple's Mac.
1987 – Windows 2.0 – Overlapping and resizable windows, keyboard and mouse enhancements
1987 – Macintosh II: first full-color Mac
1988 – OS/2 1.10 Standard Edition (SE) has GUI written by Microsoft, looks a lot like Windows 2

Interface design[edit]
Primary methods used in the interface design include prototyping and simulation.
Typical human–machine interface design consists of the following stages: interaction specification, interface software specification and prototyping:

Common practices for interaction specification include user-centered design, persona, activity-oriented design, scenario-based design, resiliency design.
Common practices for interface software specification include use cases, constrain enforcement by interaction protocols (intended to avoid use errors).
Common practices for prototyping are based on interactive design based on libraries of interface elements (controls, decoration, etc.).

Quality[edit]
All great interfaces share eight qualities or characteristics:

Clarity The interface avoids ambiguity by making everything clear through language, flow, hierarchy and metaphors for visual elements.
Concision[11] It's easy to make the interface clear by over-clarifying and labeling everything, but this leads to interface bloat, where there is just too much stuff on the screen at the same time. If too many things are on the screen, finding what you're looking for is difficult, and so the interface becomes tedious to use. The real challenge in making a great interface is to make it concise and clear at the same time.
Familiarity[12] Even if someone uses an interface for the first time, certain elements can still be familiar. Real-life metaphors can be used to communicate meaning.
Responsiveness[13] A good interface should not feel sluggish. This means that the interface should provide good feedback to the user about what's happening and whether the user's input is being successfully processed.
Consistency[14] Keeping your interface consistent across your application is important because it allows users to recognize usage patterns.
Aesthetics While you don't need to make an interface attractive for it to do its job, making something look good will make the time your users spend using your application more enjoyable; and happier users can only be a good thing.
Efficiency Time is money, and a great interface should make the user more productive through shortcuts and good design.
Forgiveness A good interface should not punish users for their mistakes but should instead provide the means to remedy them.

Principle of least astonishment[edit]
The principle of least astonishment (POLA) is a general principle in the design of all kinds of interfaces. It is based on the idea that human beings can only pay full attention to one thing at one time,[15] leading to the conclusion that novelty should be minimized.
Habit formation[edit]
If an interface is used persistently, the user will unavoidably develop habits for using the interface. The designer's role can thus be characterized as ensuring the user forms good habits. If the designer is experienced with other interfaces, they will similarly develop habits, and often make unconscious assumptions regarding how the user will interact with the interface.[15][16]
Types[edit]




HP Series 100 HP-150 Touchscreen



Direct manipulation interface is the name of a general class of user interfaces that allow users to manipulate objects presented to them, using actions that correspond at least loosely to the physical world.
Graphical user interfaces (GUI) accept input via devices such as a computer keyboard and mouse and provide articulated graphical output on the computer monitor. There are at least two different principles widely used in GUI design: Object-oriented user interfaces (OOUIs) and application oriented interfaces.[17]
Web-based user interfaces or web user interfaces (WUI) that accept input and provide output by generating web pages which are transmitted via the Internet and viewed by the user using a web browser program. Newer implementations utilize PHP, Java, JavaScript, AJAX, Apache Flex, .NET Framework, or similar technologies to provide real-time control in a separate program, eliminating the need to refresh a traditional HTML based web browser. Administrative web interfaces for web-servers, servers and networked computers are often called control panels.
Touchscreens are displays that accept input by touch of fingers or a stylus. Used in a growing amount of mobile devices and many types of point of sale, industrial processes and machines, self-service machines etc.
Command line interfaces, where the user provides the input by typing a command string with the computer keyboard and the system provides output by printing text on the computer monitor. Used by programmers and system administrators, in engineering and scientific environments, and by technically advanced personal computer users.
Touch user interface are graphical user interfaces using a touchpad or touchscreen display as a combined input and output device. They supplement or replace other forms of output with haptic feedback methods. Used in computerized simulators etc.
Hardware interfaces are the physical, spatial interfaces found on products in the real world from toasters, to car dashboards, to airplane cockpits. They are generally a mixture of knobs, buttons, sliders, switches, and touchscreens.
Attentive user interfaces manage the user attention deciding when to interrupt the user, the kind of warnings, and the level of detail of the messages presented to the user.
Batch interfaces are non-interactive user interfaces, where the user specifies all the details of the batch job in advance to batch processing, and receives the output when all the processing is done. The computer does not prompt for further input after the processing has started.
Conversational interfaces enable users to command the computer with plain text English (e.g., via text messages, or chatbots) or voice commands, instead of graphic elements. These interfaces often emulate human-to-human conversations.[18]
Conversational interface agents attempt to personify the computer interface in the form of an animated person, robot, or other character (such as Microsoft's Clippy the paperclip), and present interactions in a conversational form.
Crossing-based interfaces are graphical user interfaces in which the primary task consists in crossing boundaries instead of pointing.
Gesture interfaces are graphical user interfaces which accept input in a form of hand gestures, or mouse gestures sketched with a computer mouse or a stylus.
Holographic user interfaces provide input to electronic or electro-mechanical devices by passing a finger through reproduced holographic images of what would otherwise be tactile controls of those devices, floating freely in the air, detected by a wave source and without tactile interaction.
Intelligent user interfaces are human-machine interfaces that aim to improve the efficiency, effectiveness, and naturalness of human-machine interaction by representing, reasoning, and acting on models of the user, domain, task, discourse, and media (e.g., graphics, natural language, gesture).
Motion tracking interfaces monitor the user's body motions and translate them into commands, currently being developed by Apple.[19]
Multi-screen interfaces, employ multiple displays to provide a more flexible interaction. This is often employed in computer game interaction in both the commercial arcades and more recently the handheld markets.
Non-command user interfaces, which observe the user to infer his / her needs and intentions, without requiring that he / she formulate explicit commands.[20]
Object-oriented user interfaces (OOUI) are based on object-oriented programming metaphors, allowing users to manipulate simulated objects and their properties.
Reflexive user interfaces where the users control and redefine the entire system via the user interface alone, for instance to change its command verbs. Typically this is only possible with very rich graphic user interfaces.
Search interface is how the search box of a site is displayed, as well as the visual representation of the search results.
Tangible user interfaces, which place a greater emphasis on touch and physical environment or its element.
Task-focused interfaces are user interfaces which address the information overload problem of the desktop metaphor by making tasks, not files, the primary unit of interaction.
Text-based user interfaces are user interfaces which output a text. TUIs can either contain a command-line interface or a text-based WIMP environment.
Voice user interfaces, which accept input and provide output by generating voice prompts. The user input is made by pressing keys or buttons, or responding verbally to the interface.
Natural-language interfaces – Used for search engines and on webpages. User types in a question and waits for a response.
Zero-input interfaces get inputs from a set of sensors instead of querying the user with input dialogs.[21]
Zooming user interfaces are graphical user interfaces in which information objects are represented at different levels of scale and detail, and where the user can change the scale of the viewed area in order to show more detail.

Gallery[edit]







Historic HMI in the driver's cabin of a German steam locomotive









Modern HMI in the driver's cabin of a German Intercity-Express High-Speed Train









The HMI of a toilette (in Japan)









Voice user interface of a wearable computer (here: Google Glass)









HMI for audio mixing









HMI for video production









HMI of a machine for the sugar industry with pushbuttons









HMI for a Computer numerical control (CNC)









slightly newer HMI for a CNC-machine









emergency switch/panic switch




See also[edit]


Adaptive user interfaces
Brain-computer interface
Computer user satisfaction
Direct voice input
Distinguishable interfaces
Ergonomics and human factors – the study of designing objects to be better adapted to the shape of the human body
Flat design
Framebuffer
History of the GUI
Icon design
Information architecture – organizing, naming, and labelling information structures
Information visualization – the use of sensory representations of abstract data to reinforce cognition
Interaction design
Interaction technique
Interface (computer science)
Kinetic user interface
Knowledge visualization – the use of visual representations to transfer knowledge
Natural user interfaces
Ncurses, a semigraphical user interface.
Organic user interface
Post-WIMP
Tangible user interface
Unified Code for Units of Measure
Usability links
User assistance
User experience
User experience design
Useware
Virtual artifact
Virtual user interface


References[edit]



^ Griffin, Ben; Baston, Laurel. "Interfaces" (Presentation): 5. Retrieved 7 June 2014. The user interface of a mechanical system, a vehicle or an industrial installation is sometimes referred to as the human-machine interface (HMI). 
^ a b c d "User Interface Design and Ergonomics" (PDF). Course Cit 811. NATIONAL OPEN UNIVERSITY OF NIGERIA: SCHOOL OF SCIENCE AND TECHNOLOGY: 19. Retrieved 7 June 2014. In practice, the abbreviation MMI is still frequently used although some may claim that MMI stands for something different now. 
^ "Introduction Section". Recent advances in business administration. [S.l.]: Wseas. 2010. p. 190. ISBN 978-960-474-161-8. Other terms used are operator interface console (OIC) and operator interface terminal (OIT) 
^ Cipriani, Christian; Segil, Jacob; Birdwell, Jay; Weir, Richard (2014). "Dexterous control of a prosthetic hand using fine-wire intramuscular electrodes in targeted extrinsic muscles". IEEE Transactions on Neural Systems and Rehabilitation Engineering. 22 (4): 1–1. doi:10.1109/TNSRE.2014.2301234. ISSN 1534-4320. PMC 4501393 . Neural co-activations are present that in turn generate significant EMG levels and hence unintended movements in the case of the present human machine interface (HMI). 
^ Citi, Luca (2009). "Development of a neural interface for the control of a robotic hand" (PDF). Scuola Superiore Sant'Anna, Pisa, Italy: IMT Institute for Advanced Studies Lucca: 5. Retrieved 7 June 2014. 
^ Jordan, Joel. "Gaze Direction Analysis for the Investigation of Presence in Immersive Virtual Environments" (Thesis submitted for the degree of Doctor of Philosophy). University of London: Department of Computer Science: 5. Retrieved 7 June 2014. The aim of this thesis is to investigate the idea that the direction of gaze may be used as a device to detect a sense-of-presence in Immersive Virtual Environments (IVE) in some contexts. 
^ Ravi (August 2009). "Introduction of HMI". Retrieved 7 June 2014. In some circumstance computers might observe the user, and react according to their actions without specific commands. A means of tracking parts of the body is required, and sensors noting the position of the head, direction of gaze and so on have been used experimentally. This is particularly relevant to immersive interfaces. 
^ "HMI Guide". 
^ Richard, Stéphane. "Text User Interface Development Series Part One - T.U.I. Basics". Retrieved 13 June 2014. 
^ a b c McCown, Frank. "History of the Graphical User Interface (GUI)". Harding University. 
^ Raymond, Eric Steven (2003). "11". The Art of Unix Programming. Thyrsus Enterprises. Retrieved 13 June 2014. 
^ C. A. D'H Gough; R. Green; M. Billinghurst. "Accounting for User Familiarity in User Interfaces" (PDF). Retrieved 13 June 2014. 
^ Sweet, David (October 2001). "9 - Constructing A Responsive User Interface". KDE 2.0 Development. Sams Publishing. Retrieved 13 June 2014. 
^ John W. Satzinger; Lorne Olfman (March 1998). "User interface consistency across end-user applications: the effects on mental models". Journal of Management Information Systems. Managing virtual workplaces and teleworking with information technology. Armonk, NY. 14 (4): 167–193. 
^ a b Raskin, Jef (2000). The human interface : new directions for designing interactive systems (1. printing. ed.). Reading, Mass. [u.a.]: Addison Wesley. ISBN 0-201-37937-6. 
^ Udell, John (9 May 2003). "Interfaces are habit-forming". Infoworld. Retrieved 3 April 2017. 
^ Gordana Lamb. "Improve Your UI Design Process with Object-Oriented Techniques". Visual Basic Developer magazine. 2001. quote: "Table 1. Differences between the traditional application-oriented and object-oriented approaches to UI design."
^ Errett, Joshua. "As app fatigue sets in, Toronto engineers move on to chatbots". CBC. CBC/Radio-Canada. Retrieved July 4, 2016. 
^ appleinsider.com
^ Jakob Nielsen (April 1993). "Noncommand User Interfaces". Communications of the ACM. ACM Press. 36 (4): 83–99. doi:10.1145/255950.153582. 
^ Sharon, Taly, Henry Lieberman, and Ted Selker. "A zero-input interface for leveraging group experience in web browsing." Proceedings of the 8th international conference on Intelligent user interfaces. ACM, 2003.



External links[edit]



Look up user interface in Wiktionary, the free dictionary.





Wikimedia Commons has media related to User interfaces.



HCI and Interaction Design Conferences covers a wide area of user interface publications
Chapter 2. History: A brief History of user interfaces







v
t
e


Operating systems



General



Advocacy
Comparison
Forensic engineering
History
Hobbyist development
List
Timeline
Usage share





Kernel




Architectures



Exokernel
Hybrid
Microkernel
Monolithic
Rump kernel
Unikernel





Components



Device driver
Loadable kernel module
Microkernel
User space








Process management




Concepts



Context switch
Interrupt
IPC
Process
Process control block
Real-time
Thread
Time-sharing





Scheduling
algorithms



Computer multitasking
Fixed-priority preemptive
Multilevel feedback queue
Preemptive
Round-robin
Shortest job next








Memory management and
resource protection



Bus error
General protection fault
Memory protection
Paging
Protection ring
Segmentation fault
Virtual memory





Storage access and
file systems



Boot loader
Defragmentation
Device file
File attribute
Inode
Journal
Partition
Virtual file system
Virtual tape library





List



AmigaOS
Android
BeOS
BSD
Chrome OS
CP/M
DOS
GNU
Haiku
illumos
IncludeOS
iOS
Linux
Macintosh

Classic Mac OS
macOS


MINIX
MorphOS
MUSIC/SP
Nemesis
NOS
OpenVMS
ORVYL
OS/2
OSv
Pick
QNX
ReactOS
RISC OS
RSTS/E
RSX-11
RT-11
Solaris
TOPS-10/TOPS-20
TPF
Unix
Visi On
VM/CMS
VS/9
Windows
Xinu
z/OS





Miscellaneous concepts



API
Computer network
HAL
Live CD
Live USB
OS shell

CLI
GUI
TUI
VUI


PXE












v
t
e


Major fields of computer science




Note: This template roughly follows the 2012 ACM Computing Classification System.



Hardware



Printed circuit board
Peripheral
Integrated circuit
Very-large-scale integration
Energy consumption
Electronic design automation





Computer systems
organization



Computer architecture
Embedded system
Real-time computing
Dependability





Networks



Network architecture
Network protocol
Network components
Network scheduler
Network performance evaluation
Network service





Software organization



Interpreter
Middleware
Virtual machine
Operating system
Software quality





Software notations
and tools



Programming paradigm
Programming language
Compiler
Domain-specific language
Modeling language
Software framework
Integrated development environment
Software configuration management
Software library
Software repository





Software development



Software development process
Requirements analysis
Software design
Software construction
Software deployment
Software maintenance
Programming team
Open-source model





Theory of computation



Model of computation
Formal language
Automata theory
Computational complexity theory
Logic
Semantics





Algorithms



Algorithm design
Analysis of algorithms
Randomized algorithm
Computational geometry





Mathematics
of computing



Discrete mathematics
Probability
Statistics
Mathematical software
Information theory
Mathematical analysis
Numerical analysis





Information
systems



Database management system
Information storage systems
Enterprise information system
Social information systems
Geographic information system
Decision support system
Process control system
Multimedia information system
Data mining
Digital library
Computing platform
Digital marketing
World Wide Web
Information retrieval





Security



Cryptography
Formal methods
Security services
Intrusion detection system
Hardware security
Network security
Information security
Application security





Human–computer
interaction



Interaction design
Social computing
Ubiquitous computing
Visualization
Accessibility





Concurrency



Concurrent computing
Parallel computing
Distributed computing
Multithreading
Multiprocessing





Artificial
intelligence



Natural language processing
Knowledge representation and reasoning
Computer vision
Automated planning and scheduling
Search methodology
Control method
Philosophy of artificial intelligence
Distributed artificial intelligence





Machine learning



Supervised learning
Unsupervised learning
Reinforcement learning
Multi-task learning
Cross-validation





Graphics



Animation
Rendering
Image manipulation
Graphics processing unit
Mixed reality
Virtual reality
Image compression
Solid modeling





Applied
computing



E-commerce
Enterprise software
Computational mathematics
Computational physics
Computational chemistry
Computational biology
Computational social science
Computational engineering
Computational healthcare
Digital art
Electronic publishing
Cyberwarfare
Electronic voting
Video game
Word processing
Operations research
Educational technology
Document management








 Book
 Category
 Portal
WikiProject
 Commons












v
t
e


Social networking services



Websites




Personal



23snaps
aNobii
AsianAve
ASKfm
Badoo
Bebo
Cloob
Cyworld
Diaspora
Draugiem.lv
Ello
Facebook
Foursquare
Google+
Hello
Hi5
Highlight
Instagram
Keek
LiveJournal
Lifeknot
LockerDome
Mastodon
MeetMe
Miaopai
micro.blog
MixBit
Mixi
Musical.ly
Myspace
Nasza-klasa.pl
Nextdoor
OK.ru
Path
Peach
Periscope
Pinterest
Pixnet
Qzone
Readgeek
Renren
Sina Weibo
Slidely
Snapchat
Spaces
Spring.me
Streetlife
StudiVZ
Swarm
Tagged
Taringa!
tbh
Tea Party Community
Tinder
Tout
Tuenti
TV Time
Tumblr
Twitter
Untappd
VK
Whisper
Xanga
Yo





Professional



Academia.edu
Brainly
BranchOut
Edmodo
eXo Platform
IBM Connections
LinkedIn
Moodle
ResearchGate
Sciencescape (Metaα)
Solaborate
Viadeo
XING
IdeaPlane
Yammer





Defunct



App.net
Avatars United
Bolt
Capazoo
eConozco
Emojli
FitFinder
Formspring
FriendFeed
Friends Reunited
Friendster
Grono.net
Google Buzz
Heello
Hyves
iTunes Ping
iWiW
Jaiku
LunarStorm
Me2day
Meerkat
Mobli
Mugshot
Natter Social Network
Netlog
Orkut
Pheed
Piczo
PlanetAll
Posterous
Pownce
Qaiku
SixDegrees.com
So.cl
Surfbook
Tsū
tvtag
Vine
Windows Live Spaces
Wretch
Yahoo! 360°
Yahoo! Kickstart
Yahoo! Mash
Yahoo! Meme
Yik Yak





White-label services



Ning
Wall.fm








Tools



Social network analysis software
Diaspora (software)
Web 2.0 Suicide Machine





General



Comparison of software
Online identity
User profile
Virtual community
1+ million users
100+ million users





Applications



Social network advertising
Social network hosting service
Online dating service (comparison)
Mobile





User interface



Activity stream
Brand page
Like button
Hashtag
Groups
Reblogging
Polling
Internet petitions





Implications



Privacy issues
User gender difference
Use in investigations





Related concepts



Small-world experiment
Small-world network
Social network
Cybersectarianism
Panopticon





Protocols



Micropub
Webmention





Defunct



Distributed Social Networking Protocol
OpenSocial











						Retrieved from "https://en.wikipedia.org/w/index.php?title=User_interface&oldid=815241846"					Categories: User interfacesUser interface techniquesVirtual realityHuman communicationHuman–machine interactionHidden categories: Wikipedia articles needing clarification from August 2017Wikipedia articles needing clarification from January 2010All articles with specifically marked weasel-worded phrasesArticles with specifically marked weasel-worded phrases from December 2011