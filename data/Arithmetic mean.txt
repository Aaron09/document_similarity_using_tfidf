For a broader coverage related to this topic, see Mean.






This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2013) (Learn how and when to remove this template message)



In mathematics and statistics, the arithmetic mean ( /ˌærɪθˈmɛtɪk ˈmiːn/, stress on third syllable of "arithmetic"), or simply the mean or average when the context is clear, is the sum of a collection of numbers divided by the number of numbers in the collection.[1] The collection is often a set of results of an experiment, or a set of results from a survey. The term "arithmetic mean" is preferred in some contexts in mathematics and statistics because it helps distinguish it from other means, such as the geometric mean and the harmonic mean.
In addition to mathematics and statistics, the arithmetic mean is used frequently in fields such as economics, sociology, and history, and it is used in almost every academic field to some extent. For example, per capita income is the arithmetic average income of a nation's population.
While the arithmetic mean is often used to report central tendencies, it is not a robust statistic, meaning that it is greatly influenced by outliers (values that are very much larger or smaller than most of the values). Notably, for skewed distributions, such as the distribution of income for which a few people's incomes are substantially greater than most people's, the arithmetic mean may not accord with one's notion of "middle", and robust statistics, such as the median, may be a better description of central tendency.



Contents


1 Definition
2 Motivating properties
3 Contrast with median
4 Generalizations

4.1 Weighted average
4.2 Continuous probability distributions


5 Angles
6 See also
7 References
8 Further reading
9 External links



Definition[edit]
The arithmetic mean (or mean or average) is the most commonly used and readily understood measure of central tendency. In statistics, the term average refers to any of the measures of central tendency. The arithmetic mean is defined as being equal to the sum of the numerical values of each and every observation divided by the total number of observations. Symbolically, if we have a data set containing the values 




a

1


,

a

2


,
…
,

a

n




{\displaystyle a_{1},a_{2},\ldots ,a_{n}}

, then the arithmetic mean 



A


{\displaystyle A}

 is defined by the formula:





A
=


1
n



∑

i
=
1


n



a

i


=




a

1


+

a

2


+
⋯
+

a

n



n




{\displaystyle A={\frac {1}{n}}\sum _{i=1}^{n}a_{i}={\frac {a_{1}+a_{2}+\cdots +a_{n}}{n}}}



(See summation for an explanation of the summation operator).
For example, let us consider the monthly salary of 10 employees of a firm: 2500, 2700, 2400, 2300, 2550, 2650, 2750, 2450, 2600, 2400. The arithmetic mean is








2500
+
2700
+
2400
+
2300
+
2550
+
2650
+
2750
+
2450
+
2600
+
2400

10


=
2530.


{\displaystyle {\frac {2500+2700+2400+2300+2550+2650+2750+2450+2600+2400}{10}}=2530.}



If the data set is a statistical population (i.e., consists of every possible observation and not just a subset of them), then the mean of that population is called the population mean. If the data set is a statistical sample (a subset of the population), we call the statistic resulting from this calculation a sample mean.
The arithmetic mean of a variable is often denoted by a bar, for example as in 






x
¯





{\displaystyle {\bar {x}}}

 (read 



x


{\displaystyle x}

 bar), which is the mean of the 



n


{\displaystyle n}

 values 




x

1


,

x

2


,
…
,

x

n




{\displaystyle x_{1},x_{2},\ldots ,x_{n}}

.[2]
Motivating properties[edit]
The arithmetic mean has several properties that make it useful, especially as a measure of central tendency. These include:

If numbers 




x

1


,
…
,

x

n




{\displaystyle x_{1},\dotsc ,x_{n}}

 have mean 






x
¯





{\displaystyle {\bar {x}}}

, then 



(

x

1


−



x
¯



)
+
⋯
+
(

x

n


−



x
¯



)
=
0


{\displaystyle (x_{1}-{\bar {x}})+\dotsb +(x_{n}-{\bar {x}})=0}

. Since 




x

i


−



x
¯





{\displaystyle x_{i}-{\bar {x}}}

 is the distance from a given number to the mean, one way to interpret this property is as saying that the numbers to the left of the mean are balanced by the numbers to the right of the mean. The mean is the only single number for which the residuals (deviations from the estimate) sum to zero.
If it is required to use a single number as a "typical" value for a set of known numbers 




x

1


,
…
,

x

n




{\displaystyle x_{1},\dotsc ,x_{n}}

, then the arithmetic mean of the numbers does this best, in the sense of minimizing the sum of squared deviations from the typical value: the sum of 



(

x

i


−



x
¯




)

2




{\displaystyle (x_{i}-{\bar {x}})^{2}}

. (It follows that the sample mean is also the best single predictor in the sense of having the lowest root mean squared error.)[2] If the arithmetic mean of a population of numbers is desired, then the estimate of it that is unbiased is the arithmetic mean of a sample drawn from the population.

Contrast with median[edit]
The arithmetic mean may be contrasted with the median. The median is defined such that no more than half the values are larger than, and no more than half are smaller than, the median. If elements in the sample data increase arithmetically, when placed in some order, then the median and arithmetic average are equal. For example, consider the data sample 




1
,
2
,
3
,
4



{\displaystyle {1,2,3,4}}

. The average is 



2.5


{\displaystyle 2.5}

, as is the median. However, when we consider a sample that cannot be arranged so as to increase arithmetically, such as 




1
,
2
,
4
,
8
,
16



{\displaystyle {1,2,4,8,16}}

, the median and arithmetic average can differ significantly. In this case, the arithmetic average is 6.2 and the median is 4. In general, the average value can vary significantly from most values in the sample, and can be larger or smaller than most of them.
There are applications of this phenomenon in many fields. For example, since the 1980s, the median income in the United States has increased more slowly than the arithmetic average of income.[3]
Generalizations[edit]
Weighted average[edit]
Main article: Weighted average
A weighted average, or weighted mean, is an average in which some data points count more strongly than others, in that they are given more weight in the calculation. For example, the arithmetic mean of 



3


{\displaystyle 3}

 and 



5


{\displaystyle 5}

 is 






(
3
+
5
)

2


=
4


{\displaystyle {\frac {(3+5)}{2}}=4}

, or equivalently 




(


1
2


⋅
3
)

+

(


1
2


⋅
5
)

=
4


{\displaystyle \left({\frac {1}{2}}\cdot 3\right)+\left({\frac {1}{2}}\cdot 5\right)=4}

. In contrast, a weighted mean in which the first number receives, for example, twice as much weight as the second (perhaps because it is assumed to appear twice as often in the general population from which these numbers were sampled) would be calculated as 




(


2
3


⋅
3
)

+

(


1
3


⋅
5
)

=


11
3




{\displaystyle \left({\frac {2}{3}}\cdot 3\right)+\left({\frac {1}{3}}\cdot 5\right)={\frac {11}{3}}}

. Here the weights, which necessarily sum to the value one, are 



(
2

/

3
)


{\displaystyle (2/3)}

 and 



(
1

/

3
)


{\displaystyle (1/3)}

, the former being twice the latter. Note that the arithmetic mean (sometimes called the "unweighted average" or "equally weighted average") can be interpreted as a special case of a weighted average in which all the weights are equal to each other (equal to 





1
2




{\displaystyle {\frac {1}{2}}}

 in the above example, and equal to 





1
n




{\displaystyle {\frac {1}{n}}}

 in a situation with 



n


{\displaystyle n}

 numbers being averaged).
Continuous probability distributions[edit]




Comparison of mean, median and mode of two log-normal distributions with different skewness.


When a population of numbers, and any sample of data from it, could take on any of a continuous range of numbers, instead of for example just integers, then the probability of a number falling into one range of possible values could differ from the probability of falling into a different range of possible values, even if the lengths of both ranges are the same. In such a case, the set of probabilities can be described using a continuous probability distribution. The analog of a weighted average in this context, in which there are an infinitude of possibilities for the precise value of the variable, is called the mean of the probability distribution. The most widely encountered probability distribution is called the normal distribution; it has the property that all measures of its central tendency, including not just the mean but also the aforementioned median and the mode, are equal to each other. This property does not hold however, in the cases of a great many probability distributions, such as the lognormal distribution illustrated here.
Angles[edit]
Main article: Mean of circular quantities
Particular care must be taken when using cyclic data, such as phases or angles. Naïvely taking the arithmetic mean of 1° and 359° yields a result of 180°. This is incorrect for two reasons:

Firstly, angle measurements are only defined up to an additive constant of 360° (or 2π, if measuring in radians). Thus one could as easily call these 1° and −1°, or 361° and 719°, each of which gives a different average.
Secondly, in this situation, 0° (equivalently, 360°) is geometrically a better average value: there is lower dispersion about it (the points are both 1° from it, and 179° from 180°, the putative average).

In general application, such an oversight will lead to the average value artificially moving towards the middle of the numerical range. A solution to this problem is to use the optimization formulation (viz., define the mean as the central point: the point about which one has the lowest dispersion), and redefine the difference as a modular distance (i.e., the distance on the circle: so the modular distance between 1° and 359° is 2°, not 358°).
See also[edit]

Average
Fréchet mean
Generalized mean
Geometric mean
Harmonic mean
Mode
Sample mean and covariance
Standard error of the mean
Summary statistics

References[edit]



^ Jacobs, Harold R. (1994). Mathematics: A Human Endeavor (Third ed.). W. H. Freeman. p. 547. ISBN 0-7167-2426-X. 
^ a b Medhi, Jyotiprasad (1992). Statistical Methods: An Introductory Text. New Age International. pp. 53–58. ISBN 9788122404197. 
^ Paul Krugman, "The Rich, the Right, and the Facts: Deconstructing the Income Distribution Debate", 'The American Prospect'



Further reading[edit]

Huff, Darrell (1993). How to Lie with Statistics. W. W. Norton. ISBN 978-0-393-31072-6. 

External links[edit]

Calculations and comparisons between arithmetic and geometric mean of two numbers
Weisstein, Eric W. "Arithmetic Mean". MathWorld. 
Calculate the arithmetic mean of a series of numbers on fxSolver







v
t
e


Statistics






Outline
Index










Descriptive statistics







Continuous data




Center



Mean

arithmetic
geometric
harmonic


Median
Mode





Dispersion



Variance
Standard deviation
Coefficient of variation
Percentile
Range
Interquartile range





Shape



Central limit theorem
Moments

Skewness
Kurtosis
L-moments










Count data



Index of dispersion





Summary tables



Grouped data
Frequency distribution
Contingency table





Dependence



Pearson product-moment correlation
Rank correlation

Spearman's rho
Kendall's tau


Partial correlation
Scatter plot





Graphics



Bar chart
Biplot
Box plot
Control chart
Correlogram
Fan chart
Forest plot
Histogram
Pie chart
Q–Q plot
Run chart
Scatter plot
Stem-and-leaf display
Radar chart
















Data collection







Study design



Population
Statistic
Effect size
Statistical power
Sample size determination
Missing data





Survey methodology



Sampling

stratified
cluster


Standard error
Opinion poll
Questionnaire





Controlled experiments



Design

control
optimal


Controlled trial
Randomized
Random assignment
Replication
Blocking
Interaction
Factorial experiment





Uncontrolled studies



Observational study
Natural experiment
Quasi-experiment
















Statistical inference







Statistical theory



Population
Statistic
Probability distribution
Sampling distribution

Order statistic


Empirical distribution

Density estimation


Statistical model

Lp space


Parameter

location
scale
shape


Parametric family

Likelihood (monotone)
Location–scale family
Exponential family


Completeness
Sufficiency
Statistical functional

Bootstrap
U
V


Optimal decision

loss function


Efficiency
Statistical distance

divergence


Asymptotics
Robustness





Frequentist inference




Point estimation



Estimating equations

Maximum likelihood
Method of moments
M-estimator
Minimum distance


Unbiased estimators

Mean-unbiased minimum-variance

Rao–Blackwellization
Lehmann–Scheffé theorem


Median unbiased


Plug-in





Interval estimation



Confidence interval
Pivot
Likelihood interval
Prediction interval
Tolerance interval
Resampling

Bootstrap
Jackknife







Testing hypotheses



1- & 2-tails
Power

Uniformly most powerful test


Permutation test

Randomization test


Multiple comparisons





Parametric tests



Likelihood-ratio
Wald
Score








Specific tests







Z (normal)
Student's t-test
F





Goodness of fit



Chi-squared
Kolmogorov–Smirnov
Anderson–Darling
Lilliefors
Jarque–Bera
Normality (Shapiro–Wilk)
Likelihood-ratio test
Model selection

Cross validation
AIC
BIC







Rank statistics



Sign

Sample median


Signed rank (Wilcoxon)

Hodges–Lehmann estimator


Rank sum (Mann–Whitney)
Nonparametric anova

1-way (Kruskal–Wallis)
2-way (Friedman)
Ordered alternative (Jonckheere–Terpstra)










Bayesian inference



Bayesian probability

prior
posterior


Credible interval
Bayes factor
Bayesian estimator

Maximum posterior estimator





















Correlation
Regression analysis










Correlation



Pearson product-moment
Partial correlation
Confounding variable
Coefficient of determination





Regression analysis



Errors and residuals
Regression model validation
Mixed effects models
Simultaneous equations models
Multivariate adaptive regression splines (MARS)





Linear regression



Simple linear regression
Ordinary least squares
General linear model
Bayesian regression





Non-standard predictors



Nonlinear regression
Nonparametric
Semiparametric
Isotonic
Robust
Heteroscedasticity
Homoscedasticity





Generalized linear model



Exponential families
Logistic (Bernoulli) / Binomial / Poisson regressions





Partition of variance



Analysis of variance (ANOVA, anova)
Analysis of covariance
Multivariate ANOVA
Degrees of freedom
















Categorical / Multivariate / Time-series / Survival analysis







Categorical



Cohen's kappa
Contingency table
Graphical model
Log-linear model
McNemar's test





Multivariate



Regression
Manova
Principal components
Canonical correlation
Discriminant analysis
Cluster analysis
Classification
Structural equation model

Factor analysis


Multivariate distributions

Elliptical distributions

Normal









Time-series




General



Decomposition
Trend
Stationarity
Seasonal adjustment
Exponential smoothing
Cointegration
Structural break
Granger causality





Specific tests



Dickey–Fuller
Johansen
Q-statistic (Ljung–Box)
Durbin–Watson
Breusch–Godfrey





Time domain



Autocorrelation (ACF)

partial (PACF)


Cross-correlation (XCF)
ARMA model
ARIMA model (Box–Jenkins)
Autoregressive conditional heteroskedasticity (ARCH)
Vector autoregression (VAR)





Frequency domain



Spectral density estimation
Fourier analysis
Wavelet








Survival




Survival function



Kaplan–Meier estimator (product limit)
Proportional hazards models
Accelerated failure time (AFT) model
First hitting time





Hazard function



Nelson–Aalen estimator





Test



Log-rank test



















Applications







Biostatistics



Bioinformatics
Clinical trials / studies
Epidemiology
Medical statistics





Engineering statistics



Chemometrics
Methods engineering
Probabilistic design
Process / quality control
Reliability
System identification





Social statistics



Actuarial science
Census
Crime statistics
Demography
Econometrics
National accounts
Official statistics
Population statistics
Psychometrics





Spatial statistics



Cartography
Environmental statistics
Geographic information system
Geostatistics
Kriging














Category
Portal
Commons
 WikiProject








Statistics portal







						Retrieved from "https://en.wikipedia.org/w/index.php?title=Arithmetic_mean&oldid=816852495"					Categories: MeansHidden categories: Articles needing additional references from July 2013All articles needing additional referencesUse dmy dates from June 2013